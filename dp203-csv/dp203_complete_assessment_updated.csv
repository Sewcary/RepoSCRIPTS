"Question","Options","Correct Answer","Justification","0","0.1","0.2","0.3"
"You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account. You need to output the count of records received from the last five minutes every minute. Which windowing function should you use?","a) Session - b) Tumbling - c) Sliding - d) Hopping","d) Hopping","Hopping window generates results at regular intervals (every minute) while looking back over a sliding window (five minutes).","","","",""
"You are designing the folder structure for an Azure Data Lake Storage Gen2 container. Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month. Which folder structure should you recommend to support fast queries and simplified folder security?","a) /{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv - b) /{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv - c) /{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv - d) /{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv","d) /{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv","This structure is optimized for querying data by year and month while securing data by subject area.","","","",""
"You need to ensure that the Twitter feed data can be analyzed in the dedicated SQL pool. The solution must meet the customer sentiment analytic requirements. Which three Transact-SQL DDL commands should you run in sequence?","CREATE EXTERNAL DATA SOURCE - CREATE EXTERNAL FILE FORMAT - CREATE EXTERNAL TABLE AS SELECT","CREATE EXTERNAL DATA SOURCE - CREATE EXTERNAL FILE FORMAT - CREATE EXTERNAL TABLE AS SELECT","1. CREATE EXTERNAL DATA SOURCE: Defines the external data source from where Twitter data is pulled. 2. CREATE EXTERNAL FILE FORMAT: Specifies the format of the files storing Twitter data (CSV - Parquet - etc.). 3. CREATE EXTERNAL TABLE AS SELECT: Creates an external table and selects the data for analysis.","","","",""
"You have created an external table named ExtTable in Azure Data Explorer. Now - a database user needs to run a KQL (Kusto Query Language) query on this external table. Which of the following function should he use to refer to this table?","a) external_table() - b) access_table() - c) ext_table() - d) None of the above","a) external_table()","a) external_table(): This function is used in KQL to refer to an external table in Azure Data Explorer.","","","",""
"You are working as a data engineer in a company. Your company wants you to ingest data onto cloud data platforms in Azure. Which data processing framework will you use?","a) Online transaction processing (OLTP) - b) Extract - transform - and load (ETL) - c) Extract - load - and transform (ELT)","c) Extract - load - and transform (ELT)","ELT is the preferred framework in cloud environments like Azure - where data is loaded into a data lake or warehouse and then transformed later.","","","",""
"In structured data - you define data type at query time.","True - False","False","False: In structured data - data types are predefined in a schema at the time of database or table creation - not during query time.","","","",""
"In unstructured data - you define data type at query time.","True - False","True","True: In unstructured data - the data does not follow a predefined schema - and the data types are often interpreted at query time based on the requirements.","","","",""
"Which of the following data file formats offers the highest performance and flexibility for data analytics?","a) Yet Another Markup Language (YAML) - b) Extensible Markup Language (XML) - c) Apache Parquet - d) JavaScript Object Notation (JSON)","c) Apache Parquet","Apache Parquet is optimized for big data processing and analytics - offering high performance for columnar storage.","","","",""
"What distinguishes a Failure dependency from a Fail activity?","a) When a Fail activity is executed without error - the pipeline status is set to Success - b) When an activity bound to a Failure dependency is successful - the pipeline status is set to Success - c) Fail activities always result in a Failed pipeline run status - d) Failure dependencies are useful for cleaning up failed activities so the issue can be avoided when triggered again","c) Fail activities always result in a Failed pipeline run status","Fail activities always result in a Failed pipeline run status - regardless of conditions.","","","",""
"What information is displayed as output when you execute the PDW_SHOWSPACEUSED command?","a) The number of rows on each table for each distribution - b) The amount of physical space used for the table on each distribution - c) The amount of data read from cache for the given table per distribution - d) The amount of remaining space in the distribution","a) The number of rows on each table for each distribution - b) The amount of physical space used for the table on each distribution","The command shows the number of rows and the physical space used for the table on each distribution.","","","",""
"Why are partition keys significant when streaming data through Azure Stream Analytics?","a) Partitions are only important on datastores - not data streams - b) The number of inbound partitions must equal the number of outbound partitions - c) They optimize OLAP operations - d) They enable parallelism and concurrency","d) They enable parallelism and concurrency","Partition keys enable parallelism and concurrency in stream processing - allowing more efficient workload distribution.","","","",""
"Which Azure product is typically used for running batch jobs as part of an Azure Synapse Analytics Custom activity?","a) Azure Virtual Machine - b) Azure Functions - c) Azure App Services WebJob - d) Azure Batch","d) Azure Batch","Azure Batch is designed for running large-scale batch processing jobs in Azure Synapse.","","","",""
"What does the 'E' in ETL stand for?","a) Elicit - b) Evacuate - c) Extract - d) Excerpt","c) Extract","ETL stands for Extract - Transform - Load.","","","",""
"What distribution model is typically recommended as the most optimal for dimension tables - considering their unique characteristics and requirements?","a) Replicated - b) Round-robin - c) Heap - d) Partitioned columnstore","a) Replicated","Replicated distribution ensures optimal performance for dimension tables by replicating them across compute nodes.","","","",""
"Which Azure Storage access tier is the most expensive in terms of cost?","a) Archive - b) Hot - c) Premium - d) Cold","c) Premium","The Premium tier offers the highest performance at the highest cost.","","","",""
"In Structured data you define data type at query time.","True - False","False","Structured data requires predefined types - set during schema creation.","","","",""
"In Unstructured data you define data type at query time.","True - False","True","Unstructured data does not follow a predefined schema - and data types are often defined at query time.","","","",""
"You have created an external table named ExtTable in Azure Data Explorer. Now - a database user needs to run a KQL (Kusto Query Language) query on this external table. Which of the following function should he use to refer to this table?","a) external_table() - b) access_table() - c) ext_table() - d) None of the above","a) external_table()","a) external_table(): This function is used in KQL to refer to an external table in Azure Data Explorer. Other options do not exist in KQL.","","","",""
"You are working as a data engineer in a company. Your company wants you to ingest data onto cloud data platforms in Azure. Which data processing framework will you use?","a) Online transaction processing (OLTP) - b) Extract - transform - and load (ETL) - c) Extract - load - and transform (ELT)","c) Extract - load - and transform (ELT)","c) ELT est le cadre de traitement de données le plus utilisé dans les plateformes cloud comme Azure. Il consiste à extraire les données - à les charger dans l'entrepôt de données (comme Azure Data Lake ou Azure Synapse) - puis à les transformer après le chargement.Options incorrectes :a) Online transaction processing (OLTP) : OLTP est utilisé pour les transactions en temps réel - et non pour l'ingestion de grandes quantités de données.b) Extract - transform - and load (ETL) : Bien qu'ETL soit utilisé dans certains scénarios - ELT est privilégié dans le cloud car il permet de charger rapidement les données brutes avant de les transformer.","","","",""
"You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb. You execute a SQL query after inserting data into mytestdb.myParquetTable using Spark. What will be returned by the following query?","a) 24 - b) an error - c) a null value","b) an error","The query will result in an error because the WHERE clause uses 'name' - which does not exist. The correct column name is 'EmployeeName'.","","","",""
"When you create a temporal table in Azure SQL Database - it automatically creates a history table in the same database for capturing the historical records. Which of the following statements are true about the temporal table and history table? (Select all options that are applicable)","a) A temporal table must have 1 primary key - b) To create a temporal table - System Versioning needs to be set to On - c) To create a temporal table - System Versioning needs to be set to Off - d) It is mandatory to mention the name of the history table when you create the temporal table - e) If you don't specify the name for the history table - the default naming convention is used for the history table - f) You can specify the table constraints for the history table","b) To create a temporal table - System Versioning needs to be set to On - e) If you don't specify the name for the history table - the default naming convention is used for the history table","b) Le versioning du système doit être activé pour qu'une table temporelle fonctionne correctement et capture l'historique des enregistrements. e) Si vous ne fournissez pas de nom pour la table d'historique - SQL Server ou Azure SQL Database attribuera un nom par défaut à cette table. Les autres options sont incorrectes ou non obligatoires.","","","",""
"To create Data Factory instances - the user account that you use to sign into Azure must be a member of: (Select all options that are applicable)","a) Contributor - b) Owner role - c) Administrator of the Azure subscription - d) Write","a) Contributor - b) Owner role - c) Administrator of the Azure subscription","a) Le rôle de Contributor permet de créer et gérer des ressources dans Azure - y compris Data Factory. b) Le rôle d'Owner permet une gestion complète des ressources. c) L'administrateur d'abonnement a le contrôle total sur la gestion des ressources. d) 'Write' n'est pas un rôle valide dans Azure.","","","",""
"You need to output files from Azure Data Factory. Which file format should you use for each type of output?","Columnar format: a) Avro - b) GZIP - c) Parquet - d) TXT | JSON with a timestamp: a) Avro - b) GZIP - c) Parquet - d) TXT","Columnar format: c) Parquet | JSON with a timestamp: a) Avro","c) Parquet est un format en colonnes idéal pour l'analyse de grandes quantités de données. a) Avro est adapté aux structures JSON avec horodatage.","","","",""
"Working as a data engineer for a car sales company - you need to design an application that would accept market information as an input. Using the machine learning classification model - the application will classify the input data into two categories: a) Car models that sell more with buyers between 18-40 years and b) Car models that sell more with people above 40. What would you recommend to train the model?","a) Power BI Models - b) Text Analytics API - c) Computer Vision API - d) Apache Spark MLlib","d) Apache Spark MLlib","Apache Spark MLlib est utilisé pour entraîner des modèles de machine learning sur de grandes quantités de données. Il est parfaitement adapté à ce scénario de classification basé sur les préférences de vente.","","","",""
"You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a session window that uses a timeout size of 10 seconds. Does this meet the goal?","a) Yes - b) No","b) No","A session window is used to group events based on inactivity - not for fixed time intervals. In this case - a tumbling window is a better option to ensure each tweet is counted exactly once in each 10-second window.","","","",""
"You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a sliding window - and you set the window size to 10 seconds. Does this meet the goal?","a) Yes - b) No","b) No","A sliding window allows overlapping windows - meaning an event can be counted multiple times. In this case - a tumbling window is more appropriate as it ensures each event is counted exactly once.","","","",""
"You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a tumbling window - and you set the window size to 10 seconds. Does this meet the goal?","a) Yes - b) No","a) Yes","A tumbling window is the correct approach as it divides the data stream into fixed - non-overlapping intervals (10 seconds). This ensures that each tweet is counted exactly once within each window.","","","",""
"Which of the following are valid trigger types of Azure Data Factory? (Select all options that are applicable)","a) Monthly Trigger - b) Schedule Trigger - c) Overlap Trigger - d) Tumbling Window Trigger - e) Event-based Trigger","b) Schedule Trigger - d) Tumbling Window Trigger - e) Event-based Trigger","Les types de déclencheurs valides dans Azure Data Factory incluent le Schedule Trigger - Tumbling Window Trigger - et Event-based Trigger. Les déclencheurs mensuels peuvent être configurés via un Schedule Trigger - et Overlap Trigger n'existe pas.","","","",""
"You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub. You need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds. How should you complete the Stream Analytics query?","1) Pour compléter après FROM MessageStream : a) Last - b) Over - c) SYSTEM.TIMESTAMP() - d) TIMESTAMP BY CreatedAt | 2) Pour compléter dans GROUP BY : a) HOPPINGWINDOW - b) SESSIONWINDOW - c) SLIDINGWINDOW - d) TUMBLINGWINDOW","d) TIMESTAMP BY CreatedAt | d) TUMBLINGWINDOW","TIMESTAMP BY est utilisé pour spécifier la colonne contenant les horodatages des événements. TUMBLINGWINDOW est approprié pour segmenter les événements en fenêtres de 15 secondes sans chevauchement.","","","",""
"Duplicating customer content for redundancy and meeting service-level agreements (SLAs) is Azure Maintainability.","a) Yes - b) No","b) No","La duplication du contenu des clients pour la redondance est un aspect de l'Azure Availability - pas de la Maintainability. La disponibilité concerne la redondance et la tolérance aux pannes - tandis que la maintenabilité se concentre sur la gestion et la maintenance.","","","",""
"Duplicating customer content for redundancy and meeting service-level agreements (SLAs) is Azure High Availability.","a) Yes - b) No","a) Yes","Azure High Availability (HA) se concentre sur la redondance et la réplication des données pour garantir que les services restent disponibles - même en cas de défaillance. Cela aide à respecter les SLA (Service Level Agreements).","","","",""
"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Contacts. Contacts contains a column named Phone. You need to ensure that users in a specific role only see the last four digits of a phone number when querying the Phone column. What should you include in the solution?","a) Column encryption - b) Dynamic data masking - c) A default value - d) Table partitions - e) Row level security (RLS)","b) Dynamic data masking","Dynamic data masking permet de masquer certaines parties des données - comme ici pour ne montrer que les quatre derniers chiffres d’un numéro de téléphone.","","","",""
"You have an Azure Data Lake Storage Gen2 container. Data is ingested into the container - and then transformed by a data integration application. The data is NOT modified after that. Users can read files in the container but cannot modify the files. You need to design a data archiving solution that meets the following requirements.","Five-Year-old data: a) Delete the Blob - b) Move to Hot storage - c) Move to Cool storage - d) Move to Archive storage | Seven-Year-old data: a) Delete the Blob - b) Move to Hot storage - c) Move to Cool storage - d) Move to Archive storage","Move to Cool storage - Move to Archive storage","Move to Cool storage est idéal pour les données plus anciennes qui sont encore consultées occasionnellement - et Move to Archive storage est parfait pour les données qui ne sont plus accédées mais doivent être conservées à moindre coût.","","","",""
"As a data engineer you need to suggest Stream Analytics data output format to make sure that the queries from Databricks and PolyBase against the files encounter with less errors. The solution should make sure that the files can be queried fast - and the data type information is kept intact. What should you suggest?","a) JSON - b) XML - c) Avro - d) Parquet","d) Parquet","Parquet est un format de fichier orienté colonne conçu pour optimiser les performances de requête et conserver l'intégrité des types de données. Il est particulièrement adapté aux environnements analytiques tels que Databricks et PolyBase.","","","",""
"Which role works with Azure Cognitive Services - Cognitive Search - and the Bot Framework?","a) A data engineer - b) A data scientist - c) An AI engineer","c) An AI engineer","Un AI engineer travaille avec des services comme Azure Cognitive Services - Cognitive Search et le Bot Framework - qui sont essentiels pour créer des solutions d'intelligence artificielle.","","","",""
"Who performs advanced analytics to help drive value from data?","a) A data engineer - b) A data scientist - c) An AI engineer","b) A data scientist","Un data scientist utilise des méthodes analytiques avancées pour extraire des informations et générer de la valeur à partir des données - en se concentrant sur l'analyse approfondie et la modélisation prédictive.","","","",""
"""Choose the valid examples of Structured Data.""","""a) Microsoft SQL Server - b) Binary Files - c) Azure SQL Database - d) Audio Files - e) Azure SQL Data Warehouse - f) Image Files""","""a) Microsoft SQL Server, c) Azure SQL Database, e) Azure SQL Data Warehouse""","""Structured data refers to highly organized and easily searchable data, usually stored in databases with a predefined schema, such as SQL databases.""","","","",""
"""Azure Databricks encapsulates which Apache Storage technology?""","""a) Apache HDInsight - b) Apache Hadoop - c) Apache Spark""","""c) Apache Spark""","""Azure Databricks is a fast, easy, and collaborative Apache Spark-based analytics service, designed for big data and machine learning workloads.""","","","",""
"""Choose the valid examples of Structured Data.""","""a) Microsoft SQL Server - b) Binary Files - c) Azure SQL Database - d) Audio Files - e) Azure SQL Data Warehouse - f) Image Files""","""a) Microsoft SQL Server, c) Azure SQL Database, e) Azure SQL Data Warehouse""","""Structured data refers to highly organized and easily searchable data, usually stored in databases with a predefined schema, such as SQL databases.""","","","",""
"""Azure Databricks encapsulates which Apache Storage technology?""","""a) Apache HDInsight - b) Apache Hadoop - c) Apache Spark""","""c) Apache Spark""","""Azure Databricks is a fast, easy, and collaborative Apache Spark-based analytics service, designed for big data and machine learning workloads.""","","","",""
"","","","","""0        Choose the valid examples of Structured Data.
1    Azure Databricks encapsulates which Apache Sto...
Name: Question, dtype: object""","","",""
"","","","","""0    a) Microsoft SQL Server - b) Binary Files - c)...
1    a) Apache HDInsight - b) Apache Hadoop - c) Ap...
Name: Options, dtype: object""","","",""
"","","","","""0    a) Microsoft SQL Server, c) Azure SQL Database...
1                                      c) Apache Spark
Name: Correct Answer, dtype: object""","","",""
"","","","","""0    Structured data refers to highly organized and...
1    Azure Databricks is a fast, easy, and collabor...
Name: Justification, dtype: object""","","",""
"","","","","","""0    Azure Databricks is?
Name: Question, dtype: object""","",""
"","","","","","""0    a) data analytics platform - b) AI platform - ...
Name: Options, dtype: object""","",""
"","","","","","""0    a) data analytics platform
Name: Correct Answer, dtype: object""","",""
"","","","","","""0    Azure Databricks is an analytics platform opti...
Name: Justification, dtype: object""","",""
"","","","","","","""0    Azure Databricks is?
Name: Question, dtype: object""",""
"","","","","","","""0    a) data analytics platform - b) AI platform - ...
Name: Options, dtype: object""",""
"","","","","","","""0    a) data analytics platform
Name: Correct Answer, dtype: object""",""
"","","","","","","""0    Azure Databricks is an analytics platform opti...
Name: Justification, dtype: object""",""
"","","","","","","","""0    Which security features does Azure Databricks ...
Name: Question, dtype: object"""
"","","","","","","","""0    a) Azure Active Directory - b) Shared Access K...
Name: Options, dtype: object"""
"","","","","","","","""0    b) Shared Access Keys
Name: Correct Answer, dtype: object"""
"","","","","","","","""0    Azure Databricks integrates well with Azure Ac...
Name: Justification, dtype: object"""
"""Which security features does Azure Databricks not support?""","""a) Azure Active Directory - b) Shared Access Keys - c) Role-based access""","""b) Shared Access Keys""","""Shared Access Keys are not supported by Azure Databricks, but Azure Active Directory and Role-based access are supported.""","","","",""
"""Which of the following Azure Databricks is used for support for R, SQL, Python, Scala, and Java?""","""a) MLlib - b) GraphX - c) Spark Core API""","""c) Spark Core API""","""Spark Core API provides support for R, SQL, Python, Scala, and Java in Azure Databricks.""","","","",""
"""You configure version control for an Azure Data Factory instance as shown in the following exhibit.""","""Azure Resource Manager (ARM) templates for the pipeline’s assets are stored in: adf_publish - main - Parameterization template""","""adf_publish""","""Azure Resource Manager (ARM) templates for the pipeline’s assets are stored in the 'adf_publish' branch in Azure Data Factory.""","","","",""
