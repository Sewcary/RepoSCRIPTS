"Question","Options","Correct Answer","Justification","revoir"
"What does the 'E' in ETL stand for?","a) Elicit - b) Evacuate - c) Extract - d) Excerpt","c) Extract","ETL stands for Extract - Transform - Load.","0.0"
"You need to design a partition solution for the fact table. The solution must meet the following requirements:--- Optimize read performance when querying sales data for a single region in a given month.-- Optimize read performance when querying sales data for all regions in a given month.-- Minimize the number of partitions.--Which column should you use for partitioning?","a) Date partitioned by month - b) Product - c) Region - d) Store","b) Product","Le partitionnement par produit assure le parallélisme lors de l'interrogation des données de vente pour un mois donné dans une région ou plusieurs régions. Si on partitionne par date - toutes les ventes d'un mois donné seront dans la même partition - ce qui ne fournit pas de parallélisme. De plus - toutes les ventes d'une même région seront dans la même partition - ce qui ne permettra pas de parallélisme au sein de la région.","0.0"
"Choose the valid examples of Structured Data.","a) Microsoft SQL Server - b) Binary Files - c) Azure SQL Database - d) Audio Files - e) Azure SQL Data Warehouse - f) Image Files","a) Microsoft SQL Server - c) Azure SQL Database - e) Azure SQL Data Warehouse","Structured data refers to data that is organized in a highly structured format - often stored in relational databases. Microsoft SQL Server - Azure SQL Database - and Azure SQL Data Warehouse are all examples of systems that store structured data in tables with rows and columns.","0.0"
"You are designing a security model in Azure Synapse Analytics to support multiple companies. Which two objects should you include to ensure users only see their own company’s data?","a) Security Policy - b) Custom Role-Based Access Control - c) Function - d) Column Encryption Key - e) Asymmetric Keys","a) Security Policy - b) Custom Role-Based Access Control","a) Security Policy - b) Custom Role-Based Access Control : Corrects - car ces objets permettent de contrôler les accès par entreprise de manière granulaire.","0.0"
"You are designing a financial transaction table in Azure Synapse Analytics with 500 million rows per account type and 65 million rows per transaction month. Analysts will mostly query data by month. On which column should you partition the table?","a) Customer Segment - b) Account Type - c) Transaction Type - d) Transaction Month","d) Transaction Month","d) Transaction Month : Correct - car partitionner par mois minimise les temps de requête pour les analyses par période.","0.0"
"You have an Azure synapse analytics dedicated SQL pool named pool one and a database named DB1. DB1 contains a fact table named table one. You need to identify the extent of data skew in table one. What should you do in Synapse Studio?","a) Execute the DBCC PDW_SHOWSPACEUSED command on pool one - b) Use the sys.dm_pdw_nodes_db_partition_stats dynamic management view - c) Use the sys.dm_db_partition_stats dynamic management view - d) Execute the DBCC CHECKALLOC command on pool one","b) Use the sys.dm_pdw_nodes_db_partition_stats dynamic management view","b) Use the sys.dm_pdw_nodes_db_partition_stats dynamic management view : Correct - car cette vue permet d'analyser la répartition des données entre les nœuds pour identifier tout déséquilibre dans les tables distribuées par hachage.","0.0"
"In Azure Synapse Analytics- you want to check the overall status of the nodes in your dedicated pool to ensure they are functioning properly. Which system view should you query?","a) DBCC PDW_SHOWSPACEUSED - b) DBCC CHECKALLOC - c) sys.dm_pdw_node_status - d) sys.dm_pdw_nodes_db_partition_stats","c) sys.dm_pdw_node_status","c) sys.dm_pdw_node_status : Correct - car cette vue système permet de surveiller l'état des nœuds dans un pool dédié - ce qui est essentiel pour s'assurer qu'ils fonctionnent correctement. Les autres options sont incorrectes : a) et b) sont des commandes pour la gestion de l'espace et l'intégrité des allocations - mais elles ne fournissent pas d'informations sur l'état des nœuds. d) sys.dm_pdw_nodes_db_partition_stats est une vue utilisée pour vérifier l'état des partitions et non des nœuds.","0.0"
"By default- how long are Azure Data Factory diagnostics logs retained?","a) 15 days - b) 30 days - c) 45 days","c) 45 days","c) 45 days : Correct - car les logs de diagnostic d'Azure Data Factory sont conservés pendant 45 jours par défaut.","0.0"
"You are designing a partition strategy for a large fact table in Azure Synapse. The table has 2.4 billion records. How many partition ranges should you use for optimal performance?","a) 40 - b) 240 - c) 400 - d) 2400","a) 40","a) 40 : Correct - car cette partition offre un équilibre optimal entre compression et performances pour un grand nombre de lignes.","0.0"
"You have an Azure storage account and a data warehouse in Azure Synapse Analytics in the UK South region. You need to copy blob data from the storage account to the data warehouse using Azure Data Factory. The solution must ensure that the data remains in the UK South region at all times and minimizes administrative efforts. Which integration runtime should you use?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime","a) Azure Integration Runtime","a) Azure Integration Runtime : Correct - car il permet le déplacement des données au sein de la même région - tout en minimisant l'effort administratif.","0.0"
"Applications that publish messages to Azure Event Hub frequently will get the best performance using AMQP. Is this statement true or false?","a) True - b) False","a) True","a) True : Correct - car l'AMQP établit une connexion persistante - ce qui améliore les performances pour les publications fréquentes de messages.","0.0"
"Which Azure Storage access tier is the most expensive in terms of cost?","a) Archive - b) Hot - c) Premium - d) Cold","c) Premium","The Premium tier offers the highest performance at the highest cost.","0.0"
"You need to design an analytical storage solution for the transactional data. The solution must meet the sales transaction dataset requirements. What should you include in the solution for table type to store retail store data and table type to store promotional data?","a) Hash - b) Replicated - c) Round Robin | a) Hash - b) Replicated - c) Round Robin","b) Replicated | a) Hash","b) Replicated | a) Hash : Correct - car les tables répliquées éliminent les opérations de déplacement des données pour les requêtes de jointure sur les petites tables comme les magasins de détail. Une table de distribution hash est plus efficace pour les grandes tables comme celles des promotions.","0.0"
"How do you specify parameters when reading data in Spark?","a) Using `.option` during the read - b) Using `.parameter` during the read - c) Using `.keys` during the read","a) Using `.option` during the read","a) Using `.option` during the read : Correct - car `.option` permet de passer des paires clé-valeur pour configurer la lecture des données.","0.0"
"You have an app named App1 that contains two datasets named dataset1 and dataset2. App1 frequently queries dataset1. App1 infrequently queries dataset2. You need to prevent queries to dataset2 from affecting the buffer pool and aging out the data in dataset1. Which type of partitioning should you use?","a) functional - b) horizontal - c) table - d) vertical","d) vertical","En utilisant le partitionnement vertical - différentes parties de la base de données peuvent être isolées les unes des autres pour améliorer l'utilisation du cache.","0.0"
"You have an Azure Synapse Analytics database named DB1. You need to increase the concurrency available for DB1. Which cmdlet should you run?","a) Set-AzSqlDatabase - b) Start-AzSqlDatabaseActivity - c) Update-AzKustoDatabase - d) Update-AzSynapseSqlDatabase","a) Set-AzSqlDatabase","La commande correcte pour augmenter la concurrence sur une base de données est Set-AzSqlDatabase - car elle permet de mettre à l'échelle la base de données.  - -Les autres réponses ne sont pas correctes car : -- **b) Start-AzSqlDatabaseActivity** : Cette commande est utilisée pour démarrer une activité liée à la base de données - mais pas pour ajuster la concurrence. -- **c) Update-AzKustoDatabase** : Cette commande est spécifique à Azure Data Explorer (Kusto) et ne s'applique pas à Azure Synapse Analytics. -- **d) Update-AzSynapseSqlDatabase** : Bien que cela semble lié - cette commande n'est pas utilisée pour ajuster la concurrence dans une base de données.","0.0"
"You are testing a change to an Azure Data Factory pipeline. You need to check the change into source control without affecting other users' work in the data factory. What should you do?","a) Save the change to a forked branch in the source control project - b) Save the change to the master branch of the source control project - c) Save the changed pipeline to your workstation","a) Save the change to a forked branch in the source control project","La meilleure solution pour tester des modifications dans un pipeline sans affecter les autres utilisateurs est d'enregistrer les changements dans une branche forkée du projet de contrôle de version. Cela permet de travailler en parallèle sans perturber la branche principale. - -Les autres options ne sont pas correctes car : -- **b) Enregistrer les changements dans la branche master** : Cela affecterait potentiellement le travail des autres utilisateurs - car la branche master est généralement la version stable utilisée par tout le monde. -- **c) Enregistrer le pipeline modifié sur votre poste de travail** : Cela ne fournit pas de contrôle de version et ne permet pas de tester dans un environnement de collaboration.","0.0"
"Choose valid examples of unstructured data.","a) Microsoft SQL Server - b) Binary Files - c) Azure SQL Database - d) Audio Files - e) Azure SQL Data Warehouse - f) Image Files","b) Binary Files - d) Audio Files - f) Image Files","b) Binary Files - d) Audio Files - f) Image Files : Corrects - car ces fichiers n'ont pas de structure définie. a) Microsoft SQL Server - c) Azure SQL Database - e) Azure SQL Data Warehouse : Incorrects - car ils sont des exemples de données structurées.","0.0"
"Working as a data engineer for a car sales company - you need to design an application that would accept market information as an input. Using the machine learning classification model - the application will classify the input data into two categories: a) Car models that sell more with buyers between 18-40 years and b) Car models that sell more with people above 40. What would you recommend to train the model?","a) Power BI Models - b) Text Analytics API - c) Computer Vision API - d) Apache Spark MLlib","d) Apache Spark MLlib","Apache Spark MLlib est utilisé pour entraîner des modèles de machine learning sur de grandes quantités de données. Il est parfaitement adapté à ce scénario de classification basé sur les préférences de vente.","0.0"
"You need to implement version changes to the integration pipelines. The solution must meet the data integration requirement. In which order should you perform the actions?","a) Merge changes - b) Create a pull request - c) Create a feature branch - d) Publish changes - e) Create a repository and main branch","e) Create a repository and main branch - c) Create a feature branch - b) Create a pull request - a) Merge changes - d) Publish changes","e) Create a repository and main branch - c) Create a feature branch - b) Create a pull request - a) Merge changes - d) Publish changes : Correct - car ce flux de travail assure que les modifications dans les pipelines d'intégration sont bien contrôlées et publiées.","0.0"
"You have an external table in Azure Synapse pointing to a folder with multiple files and subfolders. If you query the external table- which files will be returned?","a) Files in the top folder only - b) Files in the top folder and subfolders - c) Only files explicitly defined in the schema","a) Files in the top folder only","a) Files in the top folder only : Correct - car seules les données du dossier racine sont renvoyées - sauf si des sous-dossiers sont explicitement spécifiés dans la requête.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool with partitioned tables. You need to overwrite a partition. What should you do?","a) Insert the data from staging to production - b) Switch the first partition from staging to production - c) Switch the first partition from production to staging - d) Update the production table from staging","b) Switch the first partition from staging to production","b) Switch the first partition from staging to production : Correct - car cela minimise le temps de chargement en basculant la partition directement sans avoir à insérer ou mettre à jour manuellement.","0.0"
"You need to trigger an Azure Data Factory pipeline when a file arrives in Azure Data Lake. Which resource provider should you enable?","a) Microsoft SQL - b) Microsoft Automation - c) Microsoft Event Grid - d) Microsoft Event Hub","c) Microsoft Event Grid","c) Microsoft Event Grid : Correct - car Azure Event Grid est conçu pour déclencher des événements basés sur le stockage - tels que l'arrivée de fichiers dans un conteneur.","0.0"
"You have an Azure Synapse Analytics database named DB1. You plan to import data into DB1. You need to maximize the performance of the data import. What should you implement?","a) functional partitioning on the source data - b) horizontal partitioning on the source data - c) table partitioning on the target database - d) vertical partitioning on the source data","b) horizontal partitioning on the source data","En utilisant le partitionnement horizontal - vous pouvez améliorer les performances du chargement des données. À mesure que plus de ressources serveur et de bande passante sont disponibles pour les fichiers sources - le processus d'importation devient plus rapide.","0.0"
"Azure Databricks is the least expensive choice when you have to store data but don't need to query it. Is this statement correct?","a) Yes - b) No","b) No","b) No : Correct - car Azure Databricks n'est pas la solution la moins chère pour stocker des données sans les interroger. a) Yes : Incorrect - car Databricks est plus coûteux que d'autres solutions comme Azure Storage.","0.0"
"By default- how are corrupt records dealt with using Spark's `spark.read.json()` function?","a) They appear in a column called `corrupt_record` - b) They are automatically deleted - c) They throw an exception and exit read operation","a) They appear in a column called `corrupt_record`","a) They appear in a column called `corrupt_record` : Correct - car Spark enregistre les lignes corrompues dans une colonne spéciale pour les examiner plus tard.","0.0"
"La vue Gantt dans Azure Data Factory permet de visualiser graphiquement les exécutions de pipeline et leur durée.","Elle permet de voir quels pipelines sont exécutés simultanément et lesquels s'exécutent à des moments différents.","La vue Gantt regroupe les exécutions par nom - annotation ou balise créée dans le pipeline.","Pour une analyse graphique des données de temps d'exécution sur une période donnée - la vue Gantt est la solution idéale pour surveiller et comprendre les performances des pipelines dans Azure Data Factory.","0.0"
"By default- how many partitions will a new Event Hub have?","a) One - b) Two - c) Three - d) Four - e) Eight","d) Four","d) Four : Correct - car par défaut - un nouvel Event Hub est configuré avec quatre partitions - offrant un bon compromis entre performance et gestion des charges.","0.0"
"You have an Azure Synapse Analytics data pipeline. You need to run the pipeline at scheduled intervals. What should you configure?","a) a control flow - b) a sink - c) a trigger - d) an activity","c) a trigger","Un **trigger** est nécessaire pour lancer l'exécution d'un pipeline à des intervalles planifiés.  - -Les autres options ne sont pas correctes car : -- **a) Control flow** : est une activité qui implémente la logique de traitement - mais ne déclenche pas le pipeline. -- **b) Sink** : représente une cible dans un flux de données - mais ne fournit pas la capacité de déclencher le pipeline. -- **d) Activity** : est une tâche dans le pipeline - mais ne peut pas déclencher directement le pipeline.","0.0"
"You have an Azure subscription that contains the following resources: An Azure Synapse Analytics workspace named workspace1- A virtual network named VNet1 that has two subnets named sn1 and sn2- Five virtual machines that are connected to sn1. You need to ensure that the virtual machines can connect to workspace1. The solution must prevent traffic from the virtual machines to workspace1 from traversing the public internet. What should you create?","a) Application gateway - b) ExpressRoute - c) Network peering - d) Private endpoint - e) Service endpoint","d) Private endpoint","Vous pouvez utiliser un point de terminaison privé (Private Endpoint) pour connecter les machines virtuelles à workspace1. Les points de terminaison de service ne sont pas disponibles pour Azure Synapse Analytics. Le réseau jumelé (Network Peering) - le gateway applicatif (Application Gateway) - et ExpressRoute ne répondent pas aux exigences de cette solution.","0.0"
"You have a partitioned table in an Azure Synapse Analytics dedicated SQL pool. What should you include in the SQL queries to maximize partition elimination?","a) JOIN - b) WHERE - c) DISTINCT - d) GROUP BY","b) WHERE","b) WHERE : Correct - car il permet d'éliminer les partitions non pertinentes pour améliorer les performances. a) - c) - d) : Incorrects - car ils ne permettent pas d'élimination de partitions.","0.0"
"What should you recommend using to secure sensitive customer contact information?","a) Transparent Data Encryption - b) Row-Level Security - c) Column-Level Security - d) Data Sensitivity Labels","c) Column-Level Security","c) Column-Level Security : Correct - car cette solution permet de restreindre l'accès aux colonnes contenant des informations sensibles - comme les informations de contact des clients.","0.0"
"You are a data architect- and the data engineering team needs to configure data synchronization between an on-premises Microsoft SQL Server database and Azure SQL Database. The synchronization process must perform an initial data synchronization with minimal downtime and then bi-directional synchronization after. Which synchronization method should you use?","a) Transactional Replication - b) Data Migration Assistant - c) SQL Server Agent Job - d) Azure SQL Data Sync","d) Azure SQL Data Sync","d) Azure SQL Data Sync : Correct - car Azure SQL Data Sync permet la synchronisation bidirectionnelle des données entre des bases de données locales et Azure SQL.","0.0"
"How do you specify parameters when reading data in Spark?","a) Using `.option` during the read - b) Using `.parameter` during the read - c) Using `.keys` during the read","a) Using `.option` during the read","a) Using `.option` during the read : Correct - car `.option` permet de passer des paires clé-valeur pour configurer la lecture des données.","0.0"
"You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container. Which resource provider should you enable?","a) Microsoft.SQL - b) Microsoft.Automation - c) Microsoft.EventGrid - d) Microsoft.EventHub","c) Microsoft.EventGrid","c) Microsoft.EventGrid : Correct - car Event Grid permet de déclencher des pipelines en réponse à l'arrivée de fichiers dans un compte de stockage.","0.0"
"You have an ELT solution that uses an Azure Storage account named datastg- an Azure HDInsight cluster- and an Azure Data Factory resource. You need to run a Hive script as an activity in the Data Factory pipeline. The solution must write the output of the script to a folder named devices in a container named data in the storage account. What should you add to the Output value in the JSON file?","a) http://data@datastg.blob.core.windows.net/devices/ - b) https://datastg.blob.core.windows.net/data/devices/ - c) wasb://data@datastg.blob.core.windows.net/devices/ - d) wasb://datastg.blob.core.windows.net/data/devices/","c) wasb://data@datastg.blob.core.windows.net/devices/","Le format wasb://data@datastg.blob.core.windows.net/devices/ est correct pour indiquer l'emplacement du fichier dans un script Hive pour écrire dans un compte de stockage Azure. La réponse d) est incorrecte car le conteneur doit être placé devant le nom du compte de stockage avec un @. Vous utilisez wasb - et non http.","0.0"
"You have an Azure subscription that contains a Delta Lake solution. The solution contains a table named employees. You need to view the contents of the employees table from 24 hours ago. You must minimize the time it takes to retrieve the data. What should you do?","a) Query the table by using TIMESTAMP AS OF - b) Query the table by using VERSION AS OF - c) Restore the database from a backup and query the table - d) Restore the table from a backup and query the table","a) Query the table by using TIMESTAMP AS OF","La commande TIMESTAMP AS OF permet de consulter les données historiques dans une table Delta Lake à un moment précis. C'est la méthode la plus rapide pour récupérer des données d'il y a 24 heures. Les autres options comme VERSION AS OF ou la restauration à partir d'une sauvegarde prendraient plus de temps et ne sont pas nécessaires dans ce cas.","0.0"
"You have an Azure Synapse Analytics workspace that includes an Azure Synapse Analytics cluster named Cluster1. You need to review the estimated execution plan for a query on a specific node of Cluster1. The query has a spid of 94 and a distribution ID of 5. Which command should you run?","a) DBCC PDW_SHOWEXECUTIONPLAN (5 - 94) - b) DBCC SHOWEXECUTIONPLAN (5 -94) - c) SELECT * FROM sys.dm_exec_query_plan WHERE spid = 94 AND distribution_id = 5 - d) SELECT * FROM sys.pdw_nodes_exec_query_plan WHERE spid = 94 AND distribution_id = 5","d) SELECT * FROM sys.pdw_nodes_exec_query_plan WHERE spid = 94 AND distribution_id = 5","La commande correcte pour examiner le plan d'exécution d'une requête sur un nœud spécifique dans Azure Synapse Analytics est SELECT * FROM sys.pdw_nodes_exec_query_plan WHERE spid = 94 AND distribution_id = 5. Cette vue système affiche le plan d'exécution des requêtes dans un environnement distribué comme Synapse Analytics.  - -Les autres réponses ne sont pas correctes car : -- **a) DBCC PDW_SHOWEXECUTIONPLAN (5 - 94)** : Cette commande n'est pas valide pour afficher les plans d'exécution dans Azure Synapse. Elle est incorrecte. -- **b) DBCC SHOWEXECUTIONPLAN (5 -94)** : DBCC SHOWEXECUTIONPLAN est utilisé dans SQL Server - mais pas dans un environnement distribué comme Azure Synapse Analytics. -- **c) SELECT * FROM sys.dm_exec_query_plan WHERE spid = 94 AND distribution_id = 5** : Cette vue système est utilisée dans SQL Server pour des environnements non distribués. Elle ne fonctionnera pas dans Azure Synapse Analytics - car Synapse utilise des nœuds distribués.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool. You need to create a table named factInternetSales that will be a large fact table in a dimensional model. FactInternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on factInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time. How should you complete the code?","a) Clustered Columnstore Index - b) Clustered Index - c) Heap","a) Clustered Columnstore Index","a) Clustered Columnstore Index: Correct - because columnstore indexes minimize data size and optimize query performance for large fact tables that need aggregation.","0.0"
"Which security feature does Azure Databricks not support?","a) Azure Active Directory - b) Shared Access Keys - c) Role-Based Access","b) Shared Access Keys","b) Shared Access Keys : Correct - car Databricks ne prend pas en charge les clés d'accès partagé. a) Azure Active Directory - c) Role-Based Access : Incorrects - car ces fonctionnalités de sécurité sont prises en charge par Azure Databricks.","0.0"
"You are working on Azure Data Lake Store Gen1 and need to know the schema of external data. Which of the following plugins would you use to know the external data schema?","a) IPV4 Lookup - b) MySQL Request - c) Pivot - d) Narrow - e) Infer Storage Schema","e) Infer Storage Schema","e) Infer Storage Schema : Correct - car ce plugin permet de déduire le schéma à partir des données de stockage externes lorsque le schéma est inconnu.","0.0"
"You are designing a database solution that will host data for multiple business units. You need to ensure that queries from one business unit do not affect the other business units. Which type of partitioning should you use?","a) functional - b) horizontal - c) table - d) vertical","a) functional","En utilisant le partitionnement fonctionnel - les différents utilisateurs de la base de données peuvent être isolés les uns des autres afin de s'assurer qu'une unité commerciale n'affecte pas une autre unité commerciale.","0.0"
"You are loading 1 million rows daily into a staging table in Azure Synapse Analytics. Which distribution method- indexing- and partitioning options should you select?","a) Hash - b) Round-robin - c) Clustered Columnstore - d) Date Partitioning","a) Hash - c) Clustered Columnstore - d) Date Partitioning","a) Hash - c) Clustered Columnstore - d) Date Partitioning : Corrects - car ces choix optimisent les performances pour les grandes quantités de données chargées quotidiennement.","0.0"
"You use PySpark in Azure Databricks to parse the following JSON input. You need to output the data in the following tabular format. How should you complete the PySpark code?","a) .select - b) .filter - c) .groupBy - d) .withColumn","a) .select | d) .withColumn","a) .select | d) .withColumn: Correct - as these functions in PySpark are used to select specific columns and add new columns to transform the data into the desired tabular format.","0.0"
"Which service should you use for ingesting millions of events per second and integrating with Azure Functions?","a) Azure Cosmos DB - b) Apache Spark - c) Azure Synapse Analytics - d) Azure Event Hubs","d) Azure Event Hubs","d) Azure Event Hubs : Correct - car il permet d'ingérer des millions d'événements par seconde et s'intègre avec Azure Functions. a) Azure Cosmos DB - b) Apache Spark - c) Azure Synapse Analytics : Incorrects - car ils sont principalement utilisés pour le stockage ou le traitement des données - et non pour l'ingestion massive en temps réel.","0.0"
"You are developing a solution that will use Azure Stream Analytics. The solution will accept input from Azure Blob storage named customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address. You also have a file in Blob storage named location_incomes that contains median incomes based on location. The file is rarely used. You need to use an address to look up a median income based on the location and must output the data to Azure SQL Database for immediate use and to Azure Data Lake Storage Gen 2 for long-term retention. The solution is to implement a Stream Analytics job that has two streaming inputs- one query- and two outputs. Does this meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car dans cette solution - il manque un input de référence pour l'utilisation rare du fichier `location_incomes` et des requêtes distinctes pour les clients en magasin et en ligne.","0.0"
"You are working as a data engineer in a company. Your company wants you to ingest data onto cloud data platforms in Azure. Which data processing framework will you use?","a) Online transaction processing (OLTP) - b) Extract - transform - and load (ETL) - c) Extract - load - and transform (ELT)","c) Extract - load - and transform (ELT)","c) ELT est le cadre de traitement de données le plus utilisé dans les plateformes cloud comme Azure. Il consiste à extraire les données - à les charger dans l'entrepôt de données (comme Azure Data Lake ou Azure Synapse) - puis à les transformer après le chargement.Options incorrectes :a) Online transaction processing (OLTP) : OLTP est utilisé pour les transactions en temps réel - et non pour l'ingestion de grandes quantités de données.b) Extract - transform - and load (ETL) : Bien qu'ETL soit utilisé dans certains scénarios - ELT est privilégié dans le cloud car il permet de charger rapidement les données brutes avant de les transformer.","0.0"
"You need to design a solution for processing real-time data using Azure HDInsight and Power BI. Which actions should you perform in sequence?","a) Create HDInsight cluster - b) Create Jupyter notebook - c) Run Spark job - d) Load to Power BI","a) Create HDInsight cluster - b) Create Jupyter notebook - c) Run Spark job - d) Load to Power BI","a) Create HDInsight cluster - b) Create Jupyter notebook - c) Run Spark job - d) Load to Power BI : Corrects - car cela représente le flux de travail pour traiter et visualiser les données en temps réel dans Power BI.","0.0"
"In Unstructured data you define data type at query time.","True - False","True","Unstructured data does not follow a predefined schema - and data types are often defined at query time.","0.0"
"Unstructured data is stored in non-relational systems commonly called NoSQL. Is this statement correct?","a) Yes - b) No","a) Yes","a) Yes : Correct - car les données non structurées - telles que les fichiers audio ou vidéo - sont généralement stockées dans des systèmes NoSQL.","0.0"
"You are writing a data import task in Azure Data Factory. You need to increase the number of rows per call to the REST sink. What should you change?","a) requestInterval to 500 - b) requestInterval to 10 -000 - c) writeBatchSize to 1 -000 - d) writeBatchSize to 100 -000","d) writeBatchSize to 100 -000","Pour augmenter le nombre de lignes par lot - il faut augmenter la valeur de writeBatchSize. La valeur par défaut pour ce paramètre est de 10 -000 - donc pour augmenter cette capacité - il est nécessaire d'utiliser une valeur supérieure à celle par défaut - comme 100 -000. Les autres options ne concernent pas directement le nombre de lignes par lot.","0.0"
"You have an Azure subscription with an Azure Storage account. You need to identify and delete blobs that were not modified in the last 100 days. You scheduled an Azure Data Factory pipeline with delete activity. Does this solution meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car une simple activité de suppression dans un pipeline Azure Data Factory ne suffit pas pour satisfaire les exigences de conformité.","0.0"
"You have an Azure Data Lake Storage Gen2 account. You grant developers Read and Write permissions by using ACLs to the files in the path \root\input\cleaned. The developers report that they cannot open the files. How should you modify the permissions to ensure that the developers can open the files?","a) Add Contributor permission to the developers. - b) Add Execute permissions to the files. - c) Grant Execute permissions to all folders. - d) Grant Execute permissions to the root folder only.","c) Grant Execute permissions to all folders.","Si vous utilisez uniquement des ACLs (et non Azure RBAC) - vous devez accorder des permissions d'exécution aux dossiers de niveau supérieur ainsi qu'à tous les dossiers dans la hiérarchie pour que les utilisateurs puissent accéder aux fichiers. Ajouter des permissions de contributeur ne donnera pas accès aux données.","0.0"
"You have two Azure Data Factory pipelines. You need to monitor the runtimes of the pipelines. What should you do?","a) From Azure Data Studio - use the performance monitor view. - b) From the Azure Monitor blade of the Azure portal - review the metrics. - c) From the Data Factory blade of the Azure portal - review the Monitor & Manage tile.","c) From the Data Factory blade of the Azure portal - review the Monitor & Manage tile.","Les temps d'exécution des pipelines existants sont disponibles dans le portail Azure - dans la section Data Factory - sous la tuile Monitor & Manage. Azure Data Studio n'est pas utilisé pour surveiller Data Factory.","0.0"
"What notebook format is used in Databricks?","a) DBC - b) .notebook - c) .spark","a) DBC","a) DBC : Correct - car DBC est le format utilisé par Databricks. b) .notebook - c) .spark : Incorrects - car ces formats de fichiers n'existent pas dans Databricks.","0.0"
"You are designing an Azure Stream Analytics job to process incoming events from sensors in a retail environment. Which type of window should you use?","a) Snapshot - b) Tumbling - c) Hopping - d) Sliding","c) Hopping","c) Hopping : Correct - car la fenêtre glissante permet un calcul des moyennes sur des intervalles de 5 minutes pour une fenêtre de 15 minutes. a) Snapshot - b) Tumbling - d) Sliding : Incorrects - car ils ne répondent pas aux besoins de chevauchement et d'intervalles.","0.0"
"You need to copy blob data from Azure Storage to Azure SQL Data Warehouse in the UK South region. Which integration runtime should you use?","a) Azure Integration Runtime - b) Self-Hosted Integration Runtime - c) Azure SSIS Integration Runtime","a) Azure Integration Runtime","a) Azure Integration Runtime : Correct - car cela permet de respecter les exigences de localisation des données et minimise l'effort administratif.","0.0"
"You are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily. You need to configure the Stream Analytics job to pick up the new reference data. What should you configure?","a) Set a refresh period - b) Enable compression on the input file - c) Use query windowing - d) Configure late arrival tolerance","a) Set a refresh period","a) Set a refresh period: Correct - because configuring the refresh period ensures that the Stream Analytics job picks up the latest version of the reference data daily.","0.0"
"You are planning to use Azure Databricks cluster for a single user. Which type of Databricks cluster should you use?","a) Standard - b) Single Node - c) High Concurrency","a) Standard","a) Standard : Correct - car le cluster standard est recommandé pour un seul utilisateur. b) - c) : Incorrects - car ils ne sont pas nécessaires pour ce cas d'utilisation simple.","0.0"
"You have an Azure Data Factory pipeline named Pipeline1. You need to ensure that Pipeline1 runs when an email is received. What should you use to create the trigger?","a) an Azure logic app - b) the Azure Synapse Analytics pipeline designer - c) the Data Factory pipeline designer","a) an Azure logic app","La solution correcte est d'utiliser une Azure Logic App pour déclencher l'exécution d'un pipeline lorsqu'un email est reçu. -- **b) the Azure Synapse Analytics pipeline designer** : Incorrect - car il n'est pas conçu pour gérer des déclencheurs basés sur des emails. -- **c) the Data Factory pipeline designer** : Incorrect - car bien que cela permette de créer des pipelines - les déclencheurs par email nécessitent une Azure Logic App.","0.0"
"Which Azure Data Factory components should you recommend using together to import daily inventory data from SQL Server to Azure Data Lake Storage?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime | a) Event-based Trigger - b) Scheduled Trigger - c) Tumbling Window Trigger | a) Copy Activity - b) Lookup Activity - c) Stored Procedure Activity","c) Self-Hosted Integration Runtime | b) Scheduled Trigger | a) Copy Activity","c) Self-Hosted Integration Runtime | b) Scheduled Trigger | a) Copy Activity : Correct - car le runtime auto-hébergé est nécessaire pour se connecter à une base SQL sur un réseau privé. Le trigger programmé permet de respecter la fréquence d'importation et l'activité de copie est utilisée pour transférer les données.","0.0"
"Which Azure service is best suited for managing and governing your data?","a) Azure Data Factory - b) Azure Purview - c) Azure Data Lake Storage","b) Azure Purview","b) Azure Purview : Correct - car Azure Purview est la solution de gouvernance des données de Microsoft - permettant de gérer et classifier les données.","0.0"
"You are a data architect- and the data engineering team needs to configure data synchronization between an on-premises Microsoft SQL Server database and Azure SQL Database. The synchronization process must perform an initial data synchronization with minimal downtime and then bi-directional synchronization after. Which synchronization method should you use?","a) Transactional Replication - b) Data Migration Assistant - c) SQL Server Agent Job - d) Azure SQL Data Sync","d) Azure SQL Data Sync","d) Azure SQL Data Sync : Correct - car Azure SQL Data Sync permet la synchronisation bidirectionnelle des données entre des bases de données locales et Azure SQL.","0.0"
"You plan to create an Azure Databricks workspace with three workloads: data engineers using Python and SQL- jobs running notebooks in multiple languages- and data scientists performing ad hoc analysis. Should you create a high concurrency cluster for each data scientist- data engineers- and a standard cluster for the jobs?","a) Yes - b) No","b) No","b) No : Correct - car un cluster standard pour chaque scientifique est suffisant - et il est inutile d'utiliser un cluster haute concurrence pour les scientifiques. Le cluster standard répond à leurs besoins.","0.0"
"You have an Azure data factory named ADF1. You currently publish all pipeline authoring changes directly to ADF1. You need to implement version control for the changes made to the pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the UX authoring canvas for ADF1. Which two actions should you perform?","a) From the UX authoring canvas - select Setup Code Repository - b) Create a Git repository - c) Create a GitHub action - d) Create an Azure Data Factory trigger - e) From the UX authoring canvas - select Publish - f) From the UX authoring canvas - run Publish All","a) From the UX authoring canvas - select Setup Code Repository - f) From the UX authoring canvas - run Publish All","a) From the UX authoring canvas - select Setup Code Repository - f) From the UX authoring canvas - run Publish All : Correct - car la configuration du dépôt de code et l'utilisation de 'Publish All' permettent de contrôler les versions des pipelines dans Azure Data Factory.","0.0"
"You have a real-time data analysis solution hosted on Microsoft Azure using Event Hub and Azure Stream Analytics. How can you optimize performance for the Stream Analytics job?","a) Implement event ordering - b) Implement user-defined functions - c) Implement query parallelization by partitioning the data input - d) Scale up the SU count for the job","c) Implement query parallelization by partitioning the data input","c) Implement query parallelization by partitioning the data input : Correct - car la parallélisation améliore les performances des jobs Stream Analytics.","0.0"
"Which transformation is used to load data into a datastore or compute resource in Azure Data Factory?","a) Source - b) Destination - c) Sync - d) Window","c) Sync","c) Sync : Correct - car la transformation Sync est utilisée pour charger les données dans une ressource de calcul ou de stockage.","0.0"
"You have an Azure subscription that contains an Azure Synapse Analytics Dedicated SQL pool named Pool1. Pool1 hosts a table named Table1. You receive JSON data from an external data source. You need to store the external data in Table1. Which T-SQL element should you use?","a) ConvertFrom-Json - b) FROM JSON - c) FOR JSON - d) OPENJSON","d) OPENJSON","La commande OPENJSON permet de convertir un document JSON en format de table dans SQL Server et Azure Synapse Analytics. Les autres options ne sont pas valides dans ce contexte. ConvertFrom-Json est utilisé en PowerShell - et FOR JSON est utilisé pour formater les résultats en JSON - tandis que FROM JSON n'existe pas en T-SQL.","0.0"
"You have an Azure Synapse Analytics database with a dimension table named 'Stores' containing information on 263 stores nationwide. The store information is retrieved in more than half of the queries. Which table geometry should you choose to improve query performance?","a) Round-robin - b) Non-clustered - c) Replicated","c) Replicated","c) Replicated : Correct - car un tableau répliqué améliore les performances des requêtes sur des tables fréquemment consultées lorsque les données sont de petite taille.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named table one. You have files that are ingested and loaded into an Azure Data Lake Storage Gen 2 container named Container1. You plan to insert data from the files in Container1 into table one and transform the data. Each row of the data in the files will produce one row in the serving layer of table one. You need to ensure that when the source data files are loaded into Container1- the date and time are stored as an additional column in table one. The solution is to use a dedicated SQL pool to create an external table that has an additional date-time column. Does this meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car une table externe dans un pool SQL dédié ne peut pas inclure automatiquement une colonne de date/heure lors de l'importation des fichiers.","0.0"
"You have an Azure Data Factory pipeline that uses Apache Spark to transform data. You need to run the pipeline. Which PowerShell cmdlet should you run?","a) Invoke-AzDataFactoryV2Pipeline - b) Invoke-AzureDataFactoryV2Pipeline - c) Start-AzDataFactoryV2Pipeline - d) Start-AzureDataFactoryV2Pipeline","a) Invoke-AzDataFactoryV2Pipeline","La commande Invoke-AzDataFactoryV2Pipeline est la bonne commande PowerShell pour démarrer un pipeline dans Azure Data Factory. Les autres options sont incorrectes car elles utilisent des préfixes de commandes incorrects ou des noms de commandes inexacts.","0.0"
"What should you recommend to prevent users outside the Perfect Pizza on-premises network from accessing the analytical data store?","a) Server-level firewall IP rule - b) Database-level virtual network rule - c) Server-level virtual network rule - d) Database-level firewall IP rule","a) Server-level firewall IP rule","a) Server-level firewall IP rule : Correct - car une règle de pare-feu au niveau du serveur est idéale pour bloquer les connexions extérieures à l'environnement réseau de l'entreprise - tout en permettant l'accès aux réseaux autorisés.","0.0"
"You have an ELT solution that uses an Azure Storage account named datastg- an Azure HDInsight cluster- and an Azure Data Factory resource. You need to run a Hive script as an activity in the Data Factory pipeline. The solution must write the output of the script to a folder named devices in a container named data in the storage account. What should you add to the Output value in the JSON file?","a) http://data@datastg.blob.core.windows.net/devices/ - b) https://datastg.blob.core.windows.net/data/devices/ - c) wasb://data@datastg.blob.core.windows.net/devices/ - d) wasb://datastg.blob.core.windows.net/data/devices/","c) wasb://data@datastg.blob.core.windows.net/devices/","Le format wasb://data@datastg.blob.core.windows.net/devices/ est correct pour indiquer l'emplacement du fichier dans un script Hive pour écrire dans un compte de stockage Azure. La réponse d) est incorrecte car le conteneur doit être placé devant le nom du compte de stockage avec un @. Vous utilisez wasb - et non http.","0.0"
"By default- how are corrupt records dealt with using Spark's `spark.read.json()` function?","a) They appear in a column called `corrupt_record` - b) They are automatically deleted - c) They throw an exception and exit read operation","a) They appear in a column called `corrupt_record`","a) They appear in a column called `corrupt_record` : Correct - car Spark enregistre les lignes corrompues dans une colonne spéciale pour les examiner plus tard.","0.0"
"What should you do to improve high availability of the real-time data processing solution?","a) Deploy a high concurrency Databricks cluster - b) Deploy an Azure Stream Analytics job and use an Azure Automation runbook to check the status of the job and restart if it stops - c) Set the Data Lake Storage to use Geo-redundant storage - d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure","d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure","d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure : Correct - car cela garantit une haute disponibilité en cas de défaillance d'un des clusters - en suivant le modèle de région appariée d'Azure.","0.0"
"You need to output files from Azure Data Factory. Which file format should you use for each type of output?","Columnar format: a) Avro - b) GZIP - c) Parquet - d) TXT | JSON with a timestamp: a) Avro - b) GZIP - c) Parquet - d) TXT","Columnar format: c) Parquet | JSON with a timestamp: a) Avro","c) Parquet est un format en colonnes idéal pour l'analyse de grandes quantités de données. a) Avro est adapté aux structures JSON avec horodatage.","0.0"
"You are developing a solution that will use Azure Stream Analytics. The solution will accept input from Azure Blob storage named customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address. You also have a file in Blob storage named location_incomes that contains median incomes based on location. The file is rarely used. You need to use an address to look up a median income based on the location and must output the data to Azure SQL Database for immediate use and to Azure Data Lake Storage Gen 2 for long-term retention. The solution is to implement a Stream Analytics job that has one streaming input- one reference input- two queries- and four outputs. Does this meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car la solution dispose d'un input de référence pour `location_incomes` - deux requêtes pour traiter les clients en magasin et en ligne - et quatre sorties - deux pour chaque type de client.","0.0"
"You are performing exploratory analysis of bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool. You execute the Transact-SQL query shown in the following exhibit. What do the query results include?","a) Only CSV files in the 'tripdata_2020' subfolder - b) All files that have file names beginning with 'tripdata_2020' - c) All CSV files that have file names containing 'tripdata_2020' - d) Only CSV files that have file names beginning with 'tripdata_2020'","d) Only CSV files that have file names beginning with 'tripdata_2020'","d) Only CSV files that have file names beginning with 'tripdata_2020': Correct - because the query filters only CSV files whose names begin with 'tripdata_2020'.","0.0"
"Azure storage is the least expensive choice when you want to store data but don't need to query it. Is this statement correct?","a) Yes - b) No","a) Yes","a) Yes : Correct - car Azure Storage est la solution la moins coûteuse pour le stockage de données sans besoin d'interrogation.","0.0"
"You have an Azure Databricks workspace in the Standard pricing tier. You need to configure auto-scaling for all-purpose clusters. What should you do first?","a) Enable Container Services - b) Upgrade to Premium Pricing Tier - c) Set Cluster Mode to High Concurrency - d) Create a Cluster Policy","b) Upgrade to Premium Pricing Tier","b) Upgrade to Premium Pricing Tier : Correct - car l'auto-scaling optimisé est uniquement disponible dans le niveau Premium. a) Enable Container Services - c) Set Cluster Mode to High Concurrency - d) Create a Cluster Policy : Incorrects - car ils n'offrent pas la réduction à trois minutes nécessaire.","0.0"
"What distribution model is typically recommended as the most optimal for dimension tables - considering their unique characteristics and requirements?","a) Replicated - b) Round-robin - c) Heap - d) Partitioned columnstore","a) Replicated","Replicated distribution ensures optimal performance for dimension tables by replicating them across compute nodes.","0.0"
"What is the default port for connecting to an enterprise data warehouse in Azure Synapse Analytics?","a) TCP 1344 - b) UDP 1433 - c) TCP 1433","c) TCP 1433","c) TCP 1433 : Correct - car c'est le port par défaut pour SQL Server et Azure Synapse.","0.0"
"You have an Azure Data Lake Storage account named store.dfs.core.windows.net and an Apache Spark notebook named Notebook1. You plan to use Notebook1 to load and transform data in store.dfs.core.windows.net. You need to configure the connection string for Notebook1. Which URI should you use?","a) adf://container@store.dfs.core.windows.net/products.csv - b) abfss://container@store.dfs.core.windows.net/products.csv - c) dbfs://container@store.dfs.core.windows.net/products.csv - d) https://container@store.dfs.core.windows.net/products.csv","b) abfss://container@store.dfs.core.windows.net/products.csv","Le protocole abfss:// est utilisé pour accéder à un compte Azure Data Lake Storage à partir d'un notebook Spark - en utilisant le Azure Blob Filesystem driver (ABFS). Les autres options - telles que adf:// - dbfs:// - et https:// - ne sont pas adaptées pour cette connexion.","0.0"
"Which table types should be used for dimension and fact tables in Azure Synapse Analytics?","a) Hash Distributed - b) Replicated","b) Replicated for dimension tables - a) Hash Distributed for fact tables","b) Replicated for dimension tables - a) Hash Distributed for fact tables : Corrects - car les petites tables de dimension sont répliquées pour de meilleures performances - tandis que les tables de faits sont distribuées par hachage pour traiter de grandes quantités de données.","0.0"
"You have an Azure data factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2. ADF1 contains two pipelines- P1 and P2. Pipeline P1 copies data from a non-partitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen 2 account. Pipeline P2 copies data from text-delimited files in Azure Data Lake Storage Gen 2 to a non-partitioned table in WS2. You need to configure P1 and P2 to maximize parallelism and performance. Which dataset setting should you configure for the copy activity in each pipeline?","a) For P1 - set PolyBase; for P2 - set Bulk Insert - b) For P1 - set Bulk Insert; for P2 - set PolyBase - c) For P1 - set PolyBase; for P2 - set PolyBase - d) For P1 - set Bulk Insert; for P2 - set Bulk Insert","c) For P1 - set PolyBase; for P2 - set PolyBase","c) For P1 - set PolyBase; for P2 - set PolyBase : Correct - car PolyBase permet une performance maximale en important des données à partir de fichiers texte ou de tables SQL sans partition.","0.0"
"You need to ensure that the Twitter feed data can be analyzed in the dedicated SQL pool. The solution must meet the customer sentiment analytic requirements. Which three Transact-SQL DDL commands should you run in sequence?","CREATE EXTERNAL DATA SOURCE - CREATE EXTERNAL FILE FORMAT - CREATE EXTERNAL TABLE AS SELECT","CREATE EXTERNAL DATA SOURCE - CREATE EXTERNAL FILE FORMAT - CREATE EXTERNAL TABLE AS SELECT","1. CREATE EXTERNAL DATA SOURCE: Defines the external data source from where Twitter data is pulled. 2. CREATE EXTERNAL FILE FORMAT: Specifies the format of the files storing Twitter data (CSV - Parquet - etc.). 3. CREATE EXTERNAL TABLE AS SELECT: Creates an external table and selects the data for analysis.","0.0"
"You have an Azure Data Factory pipeline named Pipeline1. Pipeline1 executes many API write operations every time it runs. Pipeline1 is scheduled to run every five minutes. After executing Pipeline1 10 times- you notice a throttling issue in the logs. You need to ensure that you can run Pipeline1 every five minutes. What should you do?","a) Change the compute size to large - b) Create a new integration runtime and a new Pipeline as a copy of Pipeline1. Configure both pipelines to run every 10 minutes - five minutes apart - c) Create a second trigger and set each trigger to run every 10 minutes - five minutes apart - d) Create another pipeline in the data factory and schedule each pipeline to run every 10 minutes - five minutes apart","b) Create a new integration runtime and a new Pipeline as a copy of Pipeline1. Configure both pipelines to run every 10 minutes - five minutes apart","La solution correcte consiste à créer un nouveau runtime d'intégration et un pipeline identique - puis à les faire fonctionner alternativement toutes les 10 minutes. Cela permet d'éviter les limitations de concurrence rencontrées par un seul pipeline. - -Les autres options ne résolvent pas le problème : -- **a) Change the compute size to large** : L'augmentation de la taille de calcul n'aura pas d'impact sur les limites de runtime d'intégration. -- **c) Create a second trigger and set each trigger to run every 10 minutes - five minutes apart** : Le déclenchement supplémentaire ne résoudra pas la limitation de concurrence si le runtime est saturé. -- **d) Create another pipeline in the data factory and schedule each pipeline to run every 10 minutes - five minutes apart** : Même si les pipelines alternent - ils partagent toujours les mêmes limitations de runtime d'intégration.","0.0"
"You have created an external table named ExtTable in Azure Data Explorer. Now a database user needs to run a KQL (Kusto Query Language) query on this external table. Which of the following function should he use to refer to this table?","a) external_table() - b) access_table() - c) ext_table() - d) None of the above","a) external_table()","a) external_table(): This function is used in KQL to refer to an external table in Azure Data Explorer. Other options do not exist in KQL.","0.0"
"You are developing a solution that will use Azure Stream Analytics. The solution will accept input from Azure Blob storage named customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address. You also have a file in Blob storage named location_incomes that contains median incomes based on location. The file is rarely used. You need to use an address to look up a median income based on the location and must output the data to Azure SQL Database for immediate use and to Azure Data Lake Storage Gen 2 for long-term retention. The solution is to implement a Stream Analytics job that has one query and two outputs. Does this meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car il manque l'input de référence nécessaire pour les revenus basés sur la localisation dans la solution.","0.0"
"You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account. You need to output the count of records received from the last five minutes every minute. Which windowing function should you use?","a) Session - b) Tumbling - c) Sliding - d) Hopping","d) Hopping","Hopping window generates results at regular intervals (every minute) while looking back over a sliding window (five minutes)","0.0"
"To create Data Factory instances - the user account that you use to sign into Azure must be a member of: (Select all options that are applicable)","a) Contributor - b) Owner role - c) Administrator of the Azure subscription - d) Write","a) Contributor - b) Owner role - c) Administrator of the Azure subscription","a) Le rôle de Contributor permet de créer et gérer des ressources dans Azure - y compris Data Factory. b) Le rôle d'Owner permet une gestion complète des ressources. c) L'administrateur d'abonnement a le contrôle total sur la gestion des ressources. d) 'Write' n'est pas un rôle valide dans Azure.","0.0"
"You have an Azure subscription that is linked to a hybrid Azure Active Directory tenant. The subscription contains an Azure Synapse Analytics SQL pool named Pool1. You need to recommend an authentication solution for Pool1 that supports multi-factor authentication (MFA) and database-level authentication. Which solution should you include?","a) Azure AD authentication - b) Microsoft SQL Server authentication - c) Passwordless authentication - d) Windows authentication | a) Contained database users - b) Application roles - c) Database roles - d) Microsoft SQL Server logins","a) Azure AD authentication | a) Contained database users","a) Azure AD authentication | a) Contained database users : Correct - car Azure AD prend en charge l'authentification multifactorielle (MFA) - et les utilisateurs de bases de données contenues permettent une authentification au niveau de la base de données.","0.0"
"By default- how are corrupt records dealt with using Spark's `spark.read.json()` function?","a) They appear in a column called `corrupt_record` - b) They are automatically deleted - c) They throw an exception and exit read operation","a) They appear in a column called `corrupt_record`","a) They appear in a column called `corrupt_record` : Correct - car Spark enregistre les lignes corrompues dans une colonne spéciale pour les examiner plus tard.","0.0"
"An on-premises data warehouse has two fact tables- one for sales and one for invoices. Sales data has 600 GB- with date key- product key- and region key. Invoice data has 6 GB with date key- product key- and region key. The data warehouse queries take too long to complete- so you plan to migrate to Azure Synapse Analytics. You need to optimize query performance and minimize processing skew. Which distribution type and distribution column should you recommend for each table?","a) Hash distributed - ProductKey - b) Hash distributed - RegionKey - c) Round-robin distributed - DateKey - d) Hash distributed - DateKey","a) Hash distributed - ProductKey for Sales Table - b) Hash distributed - RegionKey for Invoice Table","a) Hash distributed - ProductKey for Sales Table - b) Hash distributed - RegionKey for Invoice Table : Correct - car le hachage par `ProductKey` pour les ventes optimise les jointures et le hachage par `RegionKey` pour les factures optimise les regroupements.","0.0"
"You are implementing an Azure Data Lake Gen 2 storage account and need to ensure data is accessible for both read and write operations even if an entire data center (zonal or non-zonal) becomes unavailable. Which replication option should you use?","a) Locally Redundant Storage (LRS) - b) Zone Redundant Storage (ZRS) - c) Geo-Redundant Storage (GRS) - d) Geo-Zone Redundant Storage (GZRS)","b) Zone Redundant Storage (ZRS)","b) ZRS : Correct - car il offre une redondance à moindre coût tout en garantissant une disponibilité dans plusieurs zones. a) LRS - c) GRS - d) GZRS : Incorrects - car ils sont soit plus coûteux - soit ne répondent pas aux exigences de la question.","0.0"
"The open-source world offers four types of NoSQL databases. Select all the options that are applicable.","a) SQL Database - b) Apache Hadoop - c) Key-Value Store - d) Document Database - e) Graph Database - f) Column Database - g) Cosmos DB - h) Azure SQL Synapse","c) Key-Value Store - d) Document Database - e) Graph Database - f) Column Database","c) Key-Value Store - d) Document Database - e) Graph Database - f) Column Database : Corrects - car ces options sont des types de bases de données NoSQL. a) SQL Database - b) Apache Hadoop - g) Cosmos DB - h) Azure SQL Synapse : Incorrects - car ces options ne sont pas des types de bases de données NoSQL.","0.0"
"You plan to implement a data lifecycle policy to identify and delete blobs not modified in the last 100 days. You apply an Azure Blob Storage lifecycle policy. Does this solution meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car Azure Blob Storage Lifecycle Policy permet de gérer efficacement la suppression des blobs en fonction de leur cycle de vie.","0.0"
"You have an Azure Data Factory named ADF1. You need to review Data Factory pipeline runtimes for the last seven days. The solution must provide a graphical view of the data. What should you use?","a) the Dashboard view of the pipeline runs - b) the List view of the pipeline runs - c) the Gantt view of the pipeline runs - d) the Overview tab of Azure Data Factory Studio","c) the Gantt view of the pipeline runs","La vue Gantt des exécutions de pipeline vous montre une vue graphique des données de temps d'exécution afin que vous puissiez voir quels pipelines s'exécutent en même temps - et lesquels s'exécutent à des moments différents.","0.0"
"What should you do to improve high availability of the real-time data processing solution?","a) Deploy a high concurrency Databricks cluster - b) Deploy an Azure Stream Analytics job and use an Azure Automation runbook to check the status of the job and restart if it stops - c) Set the Data Lake Storage to use Geo-redundant storage - d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure","d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure","d) Deploy identical Azure Stream Analytics jobs to paired regions in Azure : Correct - car cela garantit une haute disponibilité en cas de défaillance d'un des clusters - en suivant le modèle de région appariée d'Azure.","0.0"
"What should you recommend using to secure sensitive customer contact information?","a) Transparent Data Encryption - b) Row-Level Security - c) Column-Level Security - d) Data Sensitivity Labels","c) Column-Level Security","c) Column-Level Security : Correct - car cette solution permet de restreindre l'accès aux colonnes contenant des informations sensibles - comme les informations de contact des clients.","0.0"
"You are creating a new notebook in Azure Databricks. Which switch should you use to switch between languages?","a) % - b) @ - c) [] - d) ()","a) %","a) % : Correct - car pour passer d'un langage à l'autre dans un notebook Databricks - on utilise le symbole `%` suivi du nom du langage - par exemple `%python`.","0.0"
"You have an Azure Stream Analytics solution that receives data from multiple thermostats in a building. You need to write a query that returns the average temperature per device every five minutes for readings within that same five minute period. Which two windowing functions could you use?","a) HoppingWindow - b) SessionWindow - c) SlidingWindow - d) TumblingWindow","a) HoppingWindow - d) TumblingWindow","Les fenêtres tumbling (TumblingWindow) ont une période définie et peuvent agréger tous les événements pour cette même période de temps. Elles permettent donc de calculer la température moyenne toutes les cinq minutes. -- **a) HoppingWindow** : Une fenêtre de saut (HoppingWindow) permet également d'agréger des événements sur une période de temps définie mais permet un chevauchement des fenêtres - ce qui en fait une option valable. -- **b) SessionWindow** : Cette fenêtre est utilisée pour agréger des événements basés sur des sessions d'activités - elle ne convient donc pas à cette situation. -- **c) SlidingWindow** : Utilisé pour agréger des événements sans une période fixe - ce qui ne correspond pas au besoin de cette requête.","0.0"
"Which Azure Data Factory component contains the transformation logic or analysis commands?","a) Linked Service - b) Datasets - c) Activities - d) Pipelines","c) Activities","c) Activities : Correct - car les activités représentent les étapes de traitement dans un pipeline Azure Data Factory.","0.0"
"You want to ingest data from a SQL Server database hosted on an on-premises Windows server using Azure Data Factory. Which integration runtime is required?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime","c) Self-Hosted Integration Runtime","c) Self-Hosted Integration Runtime : Correct - car un runtime auto-hébergé est nécessaire pour accéder à un serveur SQL local depuis Azure Data Factory.","0.0"
"You have an Azure Storage account named account1. You need to ensure that requests to account1 can only be made from specific domains. What should you configure?","a) blob public access - b) CDN - c) CORS - d) secure transfer","c) CORS","En utilisant CORS (Cross-Origin Resource Sharing) - vous pouvez spécifier quels domaines sont autorisés à effectuer des requêtes web. Si un domaine n'est pas listé comme approuvé - la requête sera rejetée.","0.0"
"How do you connect your Spark cluster to Azure Blob?","a) By calling the .connect function on the Spark cluster - b) By mounting it - c) By calling the .connect function on Azure Blob","b) By mounting it","b) By mounting it : Correct - car vous devez monter le stockage Azure Blob pour y accéder depuis un cluster Spark.","0.0"
"You need to implement changes to meet compliance standards that require identifying and deleting blobs that were not modified in the last 100 days. You apply an expired tag to the blobs in the storage account. Does this solution meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car appliquer un tag d'expiration ne permet pas de supprimer automatiquement les blobs après une période donnée.","0.0"
"You need to ensure that the data in the container is available for read workloads in a secondary region even if an outage occurs in the primary region. Which type of data redundancy should you use?","a) Geo-Redundant Storage (GRS) - b) Read-Access Geo-Redundant Storage (RA-GRS) - c) Zone Redundant Storage (ZRS) - d) Locally Redundant Storage (LRS)","b) Read-Access Geo-Redundant Storage (RA-GRS)","b) RA-GRS : Correct - car il permet une lecture dans une région secondaire en cas de panne de la région principale. a) GRS : Incorrect - car il nécessite un basculement manuel. c) ZRS - d) LRS : Incorrects - car ils ne permettent pas une redondance régionale.","0.0"
"You need to design a data retention solution for the Twitter feed data records. The solution must meet the customer sentiment analytics requirements. Which Azure storage functionality should you include in the solution?","a) Change feed - b) Soft delete - c) Time-based retention - d) Lifecycle management","d) Lifecycle management","d) Lifecycle management : Correct - car cette fonctionnalité permet de purger automatiquement les enregistrements de données Twitter plus anciens que deux ans - répondant aux besoins de rétention de données.","0.0"
"You are designing a partition strategy for a fact table in Azure Synapse with 2.4 billion records. How many partition ranges should you use for optimal compression and performance?","a) 40 - b) 240 - c) 400 - d) 2400","a) 40","a) 40 : Correct - car cela offre le meilleur compromis entre la compression et les performances pour 2 -4 milliards de lignes.","0.0"
"Azure Databricks encapsulates which Apache Storage technology?","a) Apache HDInsight - b) Apache Hadoop - c) Apache Spark","c) Apache Spark","c) Apache Spark : Correct - car Azure Databricks est basé sur Apache Spark. a) Apache HDInsight : Incorrect - car HDInsight est une autre technologie de Microsoft pour le Big Data. b) Apache Hadoop : Incorrect - car Hadoop est un cadre de stockage de données différent de Spark.","0.0"
"You are designing an application that will store petabytes of medical imaging data. When the data is first created- it will be accessed frequently during the first week. After one month- the data must be accessible within 30 seconds- but will be accessed infrequently. After one year- the data will be accessed infrequently but must be accessible within 5 minutes. You need to select the storage strategy for the data. The solution must minimize costs. Which storage tier should you use for each time frame?","a) Hot - b) Cool - c) Archive","a) Hot | b) Cool | c) Archive","a) Hot | b) Cool | c) Archive: Correct - because hot storage is suitable for frequently accessed data - cool storage is for infrequently accessed data within 30 seconds - and archive storage is for rarely accessed data with a longer retrieval time.","0.0"
"You are designing an inventory update table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and include event date- event type ID- warehouse ID- and product category type ID columns. You identify the following usage patterns: analysts will most commonly analyze transactions for a warehouse and queries will summarize by product category type- date- and inventory event type. You need to recommend a partition strategy for the table to minimize query times. On which column should you partition the table?","a) Event type ID - b) Product category type ID - c) Event date - d) Warehouse ID","d) Warehouse ID","d) Warehouse ID : Correct - car les transactions sont principalement analysées par entrepôt - donc partitionner par Warehouse ID minimise les temps de requête.","0.0"
"You have a Delta Lake solution that contains a table named table1. You need to roll back the contents of table1 to 24 hours ago. Which command should you run?","a) ALTER TABLE employee - b) COPY INTO employee1 - c) RESTORE TABLE employee TO TIMESTAMP AS OF current_timestamp() - INTERVAL '24' HOUR - d) VACUUM employee RETAIN 24","c) RESTORE TABLE employee TO TIMESTAMP AS OF current_timestamp() - INTERVAL '24' HOUR","La commande RESTORE TABLE TO TIMESTAMP permet de restaurer une table Delta Lake à un état précédent en fonction de la date et de l'heure spécifiée. Dans ce cas - elle restaure la table à son état d'il y a 24 heures. Les autres options - comme VACUUM ou COPY INTO - ne sont pas appropriées pour restaurer une table à un moment précis.","0.0"
"You are designing an enterprise data warehouse in Azure Synapse Analytics containing a table with credit card information. What should you implement to restrict salespeople from viewing credit card details while allowing access to the rest of the data?","a) Data Masking - b) Always Encrypted - c) Column-Level Security - d) Row-Level Security","c) Column-Level Security","c) Column-Level Security : Correct - car cela permet de restreindre l'accès à certaines colonnes spécifiques - comme les informations de carte de crédit.","0.0"
"Azure Cloud Learners going for certification exams often get a not so good surprise when attempting associate level exams or expert level exams and this Panic comes in terms of case study type of questions which many of the exam givers are not prepared for. What are the guidelines for case study questions?","a) Timed separately - b) No return after moving forward - c) Case studies may contain exhibits and other resources - d) Review answers at the end","b) No return after moving forward - c) Case studies may contain exhibits and other resources","b) No return after moving forward - c) Case studies may contain exhibits and other resources : Correct - car dans les études de cas des examens Azure - une fois que vous passez à la section suivante - vous ne pouvez plus revenir en arrière. De plus - des ressources supplémentaires peuvent être fournies pour vous aider à répondre aux questions.","0.0"
"You are designing the folder structure for an Azure Data Lake Storage Gen2 container. Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month. Which folder structure should you recommend to support fast queries and simplified folder security?","a) /{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv - b) /{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv - c) /{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv - d) /{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv","d) /{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv","This structure is optimized for querying data by year and month while securing data by subject area.","0.0"
"You have an Azure Data Factory pipeline named Pipeline1. You need to send an email message if Pipeline1 fails. What should you do?","a) Create a fail activity in the pipeline and set a Failure predecessor on the activity for the last activity in Pipeline1 - b) Create a metric in the Data Factory resource - c) Create an alert in the Data Factory resource - d) Create an if condition activity in the pipeline and set a Failure predecessor on the activity for the last activity in Pipeline1","c) Create an alert in the Data Factory resource","Créer une alerte dans la ressource Data Factory permet de déclencher un email en cas d'échec du pipeline. - -Les autres réponses ne sont pas correctes car : -- **a) Créer une activité de type fail et définir un prédécesseur d'échec** : Cela génère uniquement un message d'échec - mais ne permet pas d'envoyer un email. -- **b) Créer une métrique** : Les métriques ne déclenchent pas d'événements par elles-mêmes et ne permettent pas d'envoyer des emails. -- **d) Utiliser une activité If condition et un prédécesseur d'échec** : Cela permet de définir une condition de branchement mais ne permet pas directement d'envoyer un email.","0.0"
"What is the Recovery Point Objective (RPO) of Azure Synapse Analytics?","a) 4 hours - b) 8 hours - c) 12 hours - d) 16 hours","b) 8 hours","b) 8 hours : Correct - car Azure Synapse garantit un RPO de 8 heures.","0.0"
"Your company wants to route data rows to different streams based on matching conditions. Which transformation in mapping data flow should you use?","a) Conditional Split - b) Select - c) Lookup","a) Conditional Split","a) Conditional Split : Correct - car cette transformation permet de router les données vers différents flux selon des conditions définies.","0.0"
"You are planning to use Azure Databricks cluster that runs Spark jobs on the driver node. Which cluster type should you use?","a) Standard - b) Single Node - c) High Concurrency","b) Single Node","b) Single Node : Correct - car il permet d'exécuter des tâches Spark sans avoir besoin de nœuds de travail supplémentaires.","0.0"
"You need to implement a type 3 slowly changing dimension (SCD) in an Azure Synapse Analytics dedicated SQL pool. Which two columns should you add to the table?","a) Effective Start Date - b) Current Product Category - c) Effective End Date - d) Original Product Category","b) Current Product Category - d) Original Product Category","b) Current Product Category - d) Original Product Category : Corrects - car pour un type 3 SCD - il est nécessaire de stocker deux versions d'une dimension dans des colonnes séparées.","0.0"
"You need to design an Azure Synapse Analytics dedicated SQL pool to maintain employee data and minimize query complexity. How should you model the data?","a) Temporal Table - b) SQL Graph Table - c) Degenerate Dimension Table - d) Type 2 Slowly Changing Dimension (SCD) Table","d) Type 2 Slowly Changing Dimension (SCD) Table","d) Type 2 SCD Table : Correct - car il permet de stocker les versions des données tout en minimisant la complexité des requêtes. a) Temporal Table - b) SQL Graph Table - c) Degenerate Dimension Table : Incorrects - car ils ne permettent pas une gestion efficace des versions de données.","0.0"
"You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container. Which resource provider should you enable?","a) Microsoft.SQL - b) Microsoft.Automation - c) Microsoft.EventGrid - d) Microsoft.EventHub","c) Microsoft.EventGrid","c) Microsoft.EventGrid : Correct - car Event Grid permet de déclencher des pipelines en réponse à l'arrivée de fichiers dans un compte de stockage.","0.0"
"Which Azure product is typically used for running batch jobs as part of an Azure Synapse Analytics Custom activity?","a) Azure Virtual Machine - b) Azure Functions - c) Azure App Services WebJob - d) Azure Batch","d) Azure Batch","Azure Batch is designed for running large-scale batch processing jobs in Azure Synapse.","0.0"
"You are moving data from Azure Data Lake Gen 2 to Azure Synapse Analytics. Which Azure Data Factory integration runtime should be used in a data copy activity?","a) Azure Integration Runtime - b) Self-Hosted Integration Runtime - c) Azure SSIS","a) Azure Integration Runtime","a) Azure Integration Runtime : Correct - car il est utilisé pour copier des données entre deux technologies de plateforme de données Azure. b) - c) : Incorrects - car ces runtimes ne sont pas adaptés pour des transferts entre plateformes Azure.","0.0"
"You are performing exploratory analysis of bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool. You execute a Transact-SQL query- and the query results include only the CSV files in the bus fare folder. The query assumes that the first row in the CSV is a header. Does this meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car les fichiers CSV comprennent généralement une première ligne qui est un en-tête - et cela est correctement assumé par la requête.","0.0"
"Which Azure Data Factory component contains the transformation logic or analysis commands?","a) Linked Services - b) Datasets - c) Activities - d) Pipelines","c) Activities","c) Activities : Correct - car les activités contiennent la logique de transformation dans un pipeline Azure Data Factory.","0.0"
"You have a solution that upserts data to a table in an Azure Synapse Analytics database. You need to write a single T-SQL statement to upsert the data. Which T-SQL command should you run?","a) INSERT - b) MERGE - c) SELECT INTO - d) UPDATE","b) MERGE","La commande MERGE est utilisée dans T-SQL pour effectuer des opérations d'upsert (insertion et mise à jour de données) en une seule commande. Elle compare une source de données avec une table cible et applique les modifications en fonction des correspondances. Les autres options - telles que INSERT et UPDATE - ne sont pas adaptées car elles ne permettent pas de gérer à la fois l'insertion et la mise à jour dans une seule commande.","0.0"
"You have a large fact table in Azure Synapse Analytics containing 5 billion rows. Queries against the table are slow. Which type of index should you add to provide the fastest query times?","a) Non-Clustered Columnstore - b) Clustered Columnstore - c) Non-Clustered - d) Clustered","b) Clustered Columnstore","b) Clustered Columnstore : Correct - car il améliore les performances des requêtes sur de grands ensembles de données en stockant les colonnes de manière optimisée. a) - c) - d) : Incorrects - car ces index sont moins adaptés à de grandes bases de données avec des requêtes volumineuses.","0.0"
"You are designing an Azure Databricks table that will ingest an average of 20 million streaming events per day. What should you include in the solution?","a) Partition by Date-Time Fields - b) Sync to Azure Queue Storage - c) Include a Watermark Column - d) Use JSON Format for Physical Data Storage","b) Sync to Azure Queue Storage","b) Sync to Azure Queue Storage : Correct - car cela permet de réduire les coûts de stockage et de minimiser le temps de chargement. a) - c) - d) : Incorrects - car ils augmentent la complexité ou les coûts sans avantage clair.","0.0"
"You plan to create an Azure Databricks workspace with three workloads. Should you create a standard cluster for each data scientist- high concurrency for data engineers- and a standard cluster for the jobs?","a) Yes - b) No","a) Yes","a) Yes : Correct - car cette configuration répond à tous les besoins - un cluster standard pour chaque data scientist et un cluster haute concurrence pour les ingénieurs.","0.0"
"Analysts write complex queries with multiple joins and case statements in Azure Synapse Analytics. What should you implement to minimize query times?","a) Clustered Index - b) Materialized View - c) Result Set Caching - d) Replicated Table","b) Materialized View","b) Materialized View : Correct - car les vues matérialisées améliorent les performances des requêtes complexes en stockant des résultats pré-calculés.","0.0"
"You need to monitor transactions that have rolled back in a SQL pool in Azure Synapse. Which dynamic management view should you query?","a) sys.dm_pdw_nodes_trend_database_transaction - b) sys.dm_exec_requests - c) sys.dm_exec_sessions - d) sys.dm_pdw_nodes_trend_tables","a) sys.dm_pdw_nodes_trend_database_transaction","a) sys.dm_pdw_nodes_trend_database_transaction : Correct - car cette vue permet de suivre les transactions ayant échoué ou ayant été annulées.","0.0"
"What distinguishes a Failure dependency from a Fail activity?","a) When a Fail activity is executed without error - the pipeline status is set to Success - b) When an activity bound to a Failure dependency is successful - the pipeline status is set to Success - c) Fail activities always result in a Failed pipeline run status - d) Failure dependencies are useful for cleaning up failed activities so the issue can be avoided when triggered again","c) Fail activities always result in a Failed pipeline run status","Fail activities always result in a Failed pipeline run status - regardless of conditions.","0.0"
"If an Event Hub goes offline before a consumer group can process the events it holds- those events will be lost. True or False?","a) True - b) False","b) False","b) False : Correct - car les événements sont persistants dans l'Event Hub et peuvent être traités une fois que le groupe de consommateurs reprend après l'indisponibilité.","0.0"
"You plan to create an Azure Databricks workspace with three workloads. Should you create a standard cluster for each data scientist- high concurrency for data engineers- and high concurrency for the jobs?","a) Yes - b) No","b) No","b) No : Correct - car les clusters haute concurrence pour les jobs sont inefficaces. Un cluster standard pour les jobs est plus adapté.","0.0"
"You create a data flow activity in an Azure Synapse Analytics pipeline. You plan to use the data flow to read data from a fixed-length text file. You need to create the columns from each line of the text file. The solution must ensure that the data flow only writes three of the columns to a CSV file. Which three types of tasks should you add to the data flow activity? Each correct answer presents part of the solution.","a) aggregate - b) derived column - c) flatten - d) select - e) sink","b) derived column - d) select - e) sink","Vous devez utiliser une tâche de colonne dérivée (derived column) pour extraire les colonnes de chaque ligne de texte. La tâche select sélectionne les trois colonnes à écrire dans le fichier CSV. Enfin - une tâche sink est nécessaire pour écrire les données dans le fichier CSV. Il n'y a pas besoin d'agréger (aggregate) ou d'aplatir (flatten) les données dans ce scénario.","0.0"
"You plan to perform batch processing in Azure Databricks once daily. Which cluster should you use?","a) High Concurrency - b) Interactive - c) Automated","c) Automated","c) Automated : Correct - car un cluster automatisé est utilisé pour exécuter des tâches de traitement par lots - comme spécifié dans la question.","0.0"
"You execute a pipeline that contains a stored procedure- and the procedure fails. What is the status of the pipeline run?","a) Pipeline succeeded - b) Pipeline failed - c) Pipeline skipped - d) Pipeline succeeded with failures","a) Pipeline succeeded","a) Pipeline succeeded : Correct - car même si une activité échoue - la pipeline peut être conçue pour réussir grâce à des mécanismes comme les blocs try-catch.","0.0"
"What is the correct role for someone responsible for provisioning and configuring on-premises and cloud data platform technologies?","a) Data Engineer - b) Data Scientist - c) AI Engineer","a) Data Engineer","a) Data Engineer : Correct - car un ingénieur de données est responsable de la gestion des technologies de plateforme de données - y compris la configuration. b) Data Scientist : Incorrect - le Data Scientist se concentre sur l'analyse des données et non sur la configuration. c) AI Engineer : Incorrect - car un ingénieur IA travaille principalement sur les systèmes d'IA.","0.0"
"You have an Azure Synapse job using Scala. How can you view the status of the job?","a) Synapse Studio: Monitor Spark Applications - b) Azure Monitor: Custom Query on Diagnostics Table","a) Synapse Studio: Monitor Spark Applications","a) Synapse Studio: Monitor Spark Applications : Correct - car c'est l'outil dédié pour surveiller les tâches Spark dans Azure Synapse.","0.0"
"In unstructured data - you define data type at query time.","True - False","True","True: In unstructured data - the data does not follow a predefined schema - and the data types are often interpreted at query time based on the requirements.","0.0"
"You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool. How should you complete the Transact-SQL statement?","a) Clustered Index - b) Collate - c) Distribution - d) Partition - e) Partition Function - f) Partition Schema","c) Distribution - d) Partition","c) Distribution - d) Partition : Corrects - car pour créer une table partitionnée dans Azure Synapse - vous devez spécifier une distribution des données et un partitionnement. Les autres options - comme Clustered Index ou Collate - ne sont pas pertinentes dans ce contexte.","0.0"
"You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container. Which resource provider should you enable?","a) Microsoft.SQL - b) Microsoft.Automation - c) Microsoft.EventGrid - d) Microsoft.EventHub","c) Microsoft.EventGrid","c) Microsoft.EventGrid : Correct - car Event Grid permet de déclencher des pipelines en réponse à l'arrivée de fichiers dans un compte de stockage.","0.0"
"You need to audit access to PII (personally identifiable information) in Azure Synapse Analytics. Which solution should you implement?","a) Column-Level Security - b) Dynamic Data Masking - c) Sensitivity Classification","c) Sensitivity Classification","c) Sensitivity Classification : Correct - car elle permet d'auditer l'accès aux informations sensibles dans un environnement conforme au RGPD.","0.0"
"You need to implement the surrogate key for the retail store table. The solution must meet the sales transaction dataset requirements. What should you create?","a) A table that has an identity property - b) A system-versioned temporal table - c) A user-defined sequence object - d) A table that has a foreign key constraint","a) A table that has an identity property","a) A table that has an identity property : Correct - car une clé substitut est une colonne avec un identifiant unique pour chaque ligne. L'utilisation de la propriété 'identity' crée une clé substitut simple et efficace sans affecter les performances de chargement.","0.0"
"You have an Azure subscription with an Azure Storage account. You need to identify and delete blobs that were not modified in the last 100 days. You scheduled an Azure Data Factory pipeline with delete activity. Does this solution meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car une simple activité de suppression dans un pipeline Azure Data Factory ne suffit pas pour satisfaire les exigences de conformité.","0.0"
"You have a real-time data analysis solution hosted on Microsoft Azure using Event Hub and Azure Stream Analytics. How can you optimize performance for the Stream Analytics job?","a) Implement event ordering - b) Implement user-defined functions - c) Implement query parallelization by partitioning the data input - d) Scale up the SU count for the job","c) Implement query parallelization by partitioning the data input","c) Implement query parallelization by partitioning the data input : Correct - car la parallélisation améliore les performances des jobs Stream Analytics.","0.0"
"You are a data architect- and the data engineering team needs to configure data synchronization between an on-premises Microsoft SQL Server database and Azure SQL Database. The synchronization process must perform an initial data synchronization with minimal downtime and then bi-directional synchronization after. Which synchronization method should you use?","a) Transactional Replication - b) Data Migration Assistant - c) SQL Server Agent Job - d) Azure SQL Data Sync","d) Azure SQL Data Sync","d) Azure SQL Data Sync : Correct - car Azure SQL Data Sync permet la synchronisation bidirectionnelle des données entre des bases de données locales et Azure SQL.","0.0"
"You execute a pipeline that contains a stored procedure- and the procedure fails. What is the status of the pipeline run?","a) Pipeline succeeded - b) Pipeline failed - c) Pipeline skipped - d) Pipeline succeeded with failures","a) Pipeline succeeded","a) Pipeline succeeded : Correct - car même si une activité échoue - la pipeline peut être conçue pour réussir grâce à des mécanismes comme les blocs try-catch.","0.0"
"How do you specify parameters when reading data in Spark?","a) Using `.option` during the read - b) Using `.parameter` during the read - c) Using `.keys` during the read","a) Using `.option` during the read","a) Using `.option` during the read : Correct - car `.option` permet de passer des paires clé-valeur pour configurer la lecture des données.","0.0"
"You are working as a data engineer in a company. Your company wants you to ingest data onto cloud data platforms in Azure. Which data processing framework will you use?","a) Online transaction processing (OLTP) - b) Extract - transform - and load (ETL) - c) Extract - load - and transform (ELT)","c) Extract - load - and transform (ELT)","ELT is the preferred framework in cloud environments like Azure - where data is loaded into a data lake or warehouse and then transformed later.","0.0"
"You need to label each pipeline in an Azure Data Factory with its main purpose. What should you add to each pipeline?","a) A resource tag - b) A user property - c) An annotation - d) A correlation ID","c) An annotation","c) An annotation : Correct - car les annotations permettent de filtrer et de regrouper des ressources dans Data Factory.","0.0"
"Azure Databricks is:","a) Data Analytics Platform - b) AI Platform - c) Data Injection Platform","a) Data Analytics Platform","a) Data Analytics Platform : Correct - car Azure Databricks est une plateforme d'analyse de données. b) AI Platform : Incorrect - car il ne s'agit pas d'une plateforme dédiée à l'IA. c) Data Injection Platform : Incorrect - car Azure Databricks n'est pas conçu uniquement pour l'injection de données.","0.0"
"Azure Databricks encapsulates which Apache Storage technology?","a) Apache HDInsight - b) Apache Hadoop - c) Apache Spark","c) Apache Spark","Azure Databricks is built on top of Apache Spark - which is a powerful distributed computing framework used for big data processing.","0.0"
"You have an Apache Spark pool in Azure Synapse Analytics. You run a notebook that creates a DataFrame containing a large amount of data. You need to preserve the DataFrame in memory. Which two transformations can you use? Each correct answer presents a complete solution.","a) cache() - b) persist() - c) take() - d) write()","a) cache() - b) persist()","La transformation cache() conserve les données en mémoire et est déclenchée lorsque la prochaine opération - telle que count() ou take() - est exécutée. De même - persist() conserve les données en mémoire et permet de spécifier des options de stockage comme MEMORY_ONLY ou MEMORY_AND_DISK. Les opérations take() et write() ne servent pas à conserver les données en mémoire.","0.0"
"You plan to implement a data lifecycle policy to identify and delete blobs not modified in the last 100 days. You apply an Azure Blob Storage lifecycle policy. Does this solution meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car Azure Blob Storage Lifecycle Policy permet de gérer efficacement la suppression des blobs en fonction de leur cycle de vie.","0.0"
"You have an Azure Synapse Analytics pipeline connected to an Azure SQL database. You need to use data masking to obfuscate data and limit its exposure in lower-level environments. Which three options can be used for dynamic data masking? Each correct answer presents a complete solution.","a) Always Encrypted - b) Custom String - c) Default - d) Number - e) row-level security (RLS)","b) Custom String - c) Default - d) Number","Les options Default - Number et Custom String sont valides pour implémenter le masquage dynamique des données au niveau de la base de données SQL Server. Row-level security (RLS) est une capacité de restriction d'accès - pas une capacité d'obfuscation des données. Always Encrypted est une solution d'encryption au niveau de la base de données qui nécessite un pilote sur les ordinateurs clients - et ne fait pas partie du masquage dynamique des données.","0.0"
"The application that publishes messages to Azure Event Hub frequently will get the best performance using Advanced Message Queuing Protocol (AMQP) because it establishes a persistent socket. True or False?","a) True - b) False","a) True","a) True : Correct - car AMQP établit un socket persistant et permet d'envoyer plusieurs messages avec une connexion durable - optimisant ainsi la performance.","0.0"
"You have an Azure Stream Analytics job named Job1. Job1 is configured to use one Streaming Unit (SU) and can be parallelized for up to three nodes. You need to ensure there are three nodes available for the job. What is the minimum number of SUs you should configure?","a) 3 - b) 6 - c) 18 - d) 24","c) 18","Pour chaque six SUs - un nœud est créé. Par conséquent - pour utiliser trois nœuds - il est nécessaire de configurer un minimum de 18 SUs (6 SUs par nœud - 3 nœuds = 18 SUs). -- **a) 3** : Incorrect - car trois SUs ne suffiraient pas à créer un seul nœud. -- **b) 6** : Incorrect - car cela permet de configurer seulement un nœud - pas trois. -- **d) 24** : Bien que cela soit possible - ce n'est pas le nombre minimum nécessaire. 18 est suffisant pour trois nœuds.","0.0"
"In structured data - you define data type at query time.","True - False","False","False: In structured data - data types are predefined in a schema at the time of database or table creation - not during query time.","0.0"
"You need to implement changes to meet compliance standards that require identifying and deleting blobs that were not modified in the last 100 days. You apply an expired tag to the blobs in the storage account. Does this solution meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car appliquer un tag d'expiration ne permet pas de supprimer automatiquement les blobs après une période donnée.","0.0"
"You need to design a data storage structure for the product sales transaction. The solution must meet the sales transaction dataset requirements. What should you include in the solution?","a) Hash - b) Replicated - c) Set the distribution column to Product ID - d) Set the distribution column to Month","a) Hash - c) Set the distribution column to Product ID","a) Hash - c) Set the distribution column to Product ID : Correct - car partitionner par hash sur Product ID améliore les performances des requêtes en permettant un traitement distribué des données par identifiant de produit.","0.0"
"You plan to monitor an Azure data factory using the Monitor and Manage app. You need to identify the status and duration of activities that reference a table in the source database. Which three actions should you perform in sequence?","a) From the Data Factory monitoring app - add the source user property to the activity run table - b) From the Data Factory monitoring app - add the source user property to the pipeline run table - c) From the Data Factory authoring UI - publish the pipelines - d) From the Data Factory monitoring app - add a linked service to the pipeline branch table - e) From the Data Factory authoring UI - generate a user property for the source on all activities - f) From the Data Factory authoring UI - generate a user property for the source on all datasets","e) From the Data Factory authoring UI - generate a user property for the source on all activities - a) From the Data Factory monitoring app - add the source user property to the activity run table - c) From the Data Factory authoring UI - publish the pipelines","e) From the Data Factory authoring UI - generate a user property for the source on all activities - a) From the Data Factory monitoring app - add the source user property to the activity run table - c) From the Data Factory authoring UI - publish the pipelines : Correct - car ces actions permettent de suivre les activités de la pipeline en fonction des propriétés utilisateur définies.","0.0"
"You have an Azure Data Factory pipeline named S3toDataLake1 that copies data between Amazon S3 storage and Azure Data Lake storage. You need to use the Azure Data Factory Pipeline runs dashboard to view the history of runs over a specific time range and group them by tags for S3toDataLake1. Which view should you use?","a) Activity - b) Debug - c) Gantt - d) List","c) Gantt","La vue Gantt permet de voir tous les exécutions du pipeline regroupées par nom - annotation ou balise créés dans le pipeline. Elle affiche également des barres relatives à la durée de chaque exécution.","0.0"
"You are a data engineer for Contoso. Which tool in Stream Analytics should you use to view key health metrics of your jobs?","a) Dashboards - b) Alerts - c) Diagnostics","a) Dashboards","a) Dashboards : Correct - car les tableaux de bord permettent de visualiser les métriques clés de la santé des jobs dans Stream Analytics.","0.0"
"Which Azure Databricks feature supports R- SQL- Python- Scala- and Java?","a) MLlib - b) GraphX - c) Spark Core API","c) Spark Core API","c) Spark Core API : Correct - car Spark Core prend en charge plusieurs langages (R - SQL - Python - Scala - Java). a) MLlib : Incorrect - car MLlib est une bibliothèque d'apprentissage automatique. b) GraphX : Incorrect - car GraphX est utilisé pour le traitement des graphes.","0.0"
"You have an Azure storage account and a data warehouse in Azure Synapse Analytics in the UK South region. You need to copy blob data from the storage account to the data warehouse using Azure Data Factory. The solution must ensure that the data remains in the UK South region at all times and minimizes administrative efforts. Which integration runtime should you use?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime","a) Azure Integration Runtime","a) Azure Integration Runtime : Correct - car il permet le déplacement des données au sein de la même région - tout en minimisant l'effort administratif.","0.0"
"If an Azure Event Hub goes offline before a consumer group can process the events- will those events be lost?","a) True - b) False","b) False","b) False : Correct - car les événements sont persistants dans Azure Event Hub et ne sont pas perdus même si le hub est hors ligne.","0.0"
"You plan to ingest streaming social media data using Azure Stream Analytics. What stream analytics data output format should you recommend?","a) JSON - b) Parquet - c) CSV - d) Avro","b) Parquet","b) Parquet : Correct - car Parquet est un format de fichier optimisé pour les performances de requêtes et la rétention des types de données.","0.0"
"In Structured data you define data type at query time.","True - False","False","Structured data requires predefined types - set during schema creation.","0.0"
"You have an Azure Data Factory pipeline named Pipeline1. Pipeline1 executes many API write operations every time it runs. Pipeline1 is scheduled to run every five minutes. After executing Pipeline1 10 times- you notice a throttling issue in the logs. You need to ensure that you can run Pipeline1 every five minutes. What should you do?","a) Change the compute size to large - b) Create a new integration runtime and a new Pipeline as a copy of Pipeline1. Configure both pipelines to run every 10 minutes - five minutes apart - c) Create a second trigger and set each trigger to run every 10 minutes - five minutes apart - d) Create another pipeline in the data factory and schedule each pipeline to run every 10 minutes - five minutes apart","b) Create a new integration runtime and a new Pipeline as a copy of Pipeline1. Configure both pipelines to run every 10 minutes - five minutes apart","La solution correcte consiste à créer un nouveau runtime d'intégration et un pipeline identique - puis à les faire fonctionner alternativement toutes les 10 minutes. Cela permet d'éviter les limitations de concurrence rencontrées par un seul pipeline. - -Les autres options ne résolvent pas le problème : -- **a) Change the compute size to large** : L'augmentation de la taille de calcul n'aura pas d'impact sur les limites de runtime d'intégration. -- **c) Create a second trigger and set each trigger to run every 10 minutes - five minutes apart** : Le déclenchement supplémentaire ne résoudra pas la limitation de concurrence si le runtime est saturé. -- **d) Create another pipeline in the data factory and schedule each pipeline to run every 10 minutes - five minutes apart** : Même si les pipelines alternent - ils partagent toujours les mêmes limitations de runtime d'intégration.","0.0"
"You have created an external table named ExtTable in Azure Data Explorer. Now a database user needs to run a KQL (Kusto Query Language) query on this external table. Which of the following function should he use to refer to this table?","a) external_table() - b) access_table() - c) ext_table() - d) None of the above","a) external_table()","a) external_table(): This function is used in KQL to refer to an external table in Azure Data Explorer. Other options do not exist in KQL.","0.0"
"You have an Azure Synapse Analytics database named DB1. You need to import data into DB1. The solution must minimize Azure Data Lake Storage transaction costs. Which design pattern should you use?","a) Store the data in 500-MB files - b) Store the data in 2 -000-byte files - c) Use a read-access geo-redundant storage (RA-GRS) storage account - d) Use the Avro file format - e) Use the ORC file format","a) Store the data in 500-MB files","En utilisant des fichiers plus volumineux lors de l'importation des données - les coûts de transaction peuvent être réduits. Cela est dû au fait que la lecture des fichiers est facturée avec une opération de 4 Mo - même si le fichier fait moins de 4 Mo. Pour réduire les coûts - les 4 Mo complets doivent être utilisés par lecture.","0.0"
"You are designing a financial transaction table in Azure Synapse Analytics with 500 million rows per account type and 65 million rows per transaction month. Analysts will mostly query data by month. On which column should you partition the table?","a) Customer Segment - b) Account Type - c) Transaction Type - d) Transaction Month","d) Transaction Month","d) Transaction Month : Correct - car partitionner par mois minimise les temps de requête pour les analyses par période.","0.0"
"How does Spark connect to databases like MySQL or Hive?","a) JDBC - b) ODBC - c) REST API","a) JDBC","a) JDBC : Correct - car JDBC est l'API Java utilisée pour se connecter aux bases de données comme MySQL et Hive. ODBC et REST API ne sont pas utilisés dans ce contexte.","0.0"
"You have an enterprise-wide Azure Data Lake Storage Gen 2 account. You need to ensure the SQL pool can load sales data from the Data Lake. Which three actions should you perform?","a) Add the managed identity to the sales group - b) Use the managed identity as credentials for the data load process - c) Create a shared access signature - d) Add your Azure Active Directory account to the sales group - e) Use the shared access signature as credentials - f) Create a managed identity","a) Add the managed identity to the sales group - b) Use the managed identity as credentials - f) Create a managed identity","a) - b) - f) : Corrects - car vous devez créer une identité managée - l'ajouter au groupe de vente et utiliser cette identité pour le chargement des données. Les autres options - comme les signatures d'accès partagé - ne conviennent pas ici.","0.0"
"You are planning to use Azure Databricks cluster for fine-grained sharing and resource utilization. Which cluster mode should you use?","a) Standard - b) Single Node - c) High Concurrency","c) High Concurrency","c) High Concurrency : Correct - car ce mode permet un partage précis des ressources pour une utilisation optimale.","0.0"
"What should you recommend to prevent users outside the Perfect Pizza on-premises network from accessing the analytical data store?","a) Server-level firewall IP rule - b) Database-level virtual network rule - c) Server-level virtual network rule - d) Database-level firewall IP rule","a) Server-level firewall IP rule","a) Server-level firewall IP rule : Correct - car une règle de pare-feu au niveau du serveur est idéale pour bloquer les connexions extérieures à l'environnement réseau de l'entreprise - tout en permettant l'accès aux réseaux autorisés.","0.0"
"You are designing a security model for an Azure Synapse Analytics pool that will support multiple companies. Which two objects should you include in the solution?","a) Security Policy - b) Custom Role-Based Access Control (RBAC)","a) Security Policy - b) Custom Role-Based Access Control (RBAC)","a) Security Policy - b) Custom Role-Based Access Control (RBAC) : Corrects - car ils permettent de limiter l'accès aux données par entreprise dans un environnement multi-tenant.","0.0"
"Choose valid examples of structured data.","a) Microsoft SQL Server - b) Binary Files - c) Azure SQL Database - d) Audio Files - e) Azure SQL Data Warehouse - f) Image Files","a) Microsoft SQL Server - c) Azure SQL Database - e) Azure SQL Data Warehouse","a) Microsoft SQL Server - c) Azure SQL Database - e) Azure SQL Data Warehouse : Corrects - car ces bases de données stockent des données dans un format structuré. b) Binary Files - d) Audio Files - f) Image Files : Incorrects - car ils sont des exemples de données non structurées.","0.0"
"You have a self-hosted integration runtime in Azure Data Factory. If a node becomes unavailable- what will happen to the running pipelines?","a) Fail until the node comes back online - b) Switch to another integration runtime - c) Exceed the CPU limit","a) Fail until the node comes back online","a) Fail until the node comes back online : Correct - car avec la haute disponibilité désactivée - les pipelines échouent jusqu'à ce que le nœud soit de nouveau en ligne.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named table one. Table one contains 1 billion rows- a clustered columnstore index- a hash-distributed column named ProductKey- and a SalesDate column of type date. 30 million rows will be added to table one each month. You need to partition table one based on the SalesDate column. The solution must optimize query performance and data loading. How often should you create a partition?","a) Once per month - b) Once per year - c) Once per day - d) Once per week","b) Once per year","b) Once per year : Correct - car partitionner par année permet de gérer efficacement les grands volumes de données tout en optimisant les performances de requête.","0.0"
"You are designing a monitoring solution for 500 vehicles with GPS data sent to an Azure Event Hub. What should you include in the solution?","a) Azure Stream Analytics - b) Azure Databricks - c) Azure Functions - d) Azure Event Grid","a) Azure Stream Analytics - b) Hopping - c) Point-with-Polygon Analysis","a) Azure Stream Analytics : Correct - car Azure Stream Analytics est conçu pour traiter de grands volumes de données en temps réel. b) - c) - d) : Incorrects - car ils ne répondent pas aux exigences spécifiques du problème.","0.0"
"Which of the following are valid trigger types of Azure Data Factory? (Select all options that are applicable)","a) Monthly Trigger - b) Schedule Trigger - c) Overlap Trigger - d) Tumbling Window Trigger - e) Event-based Trigger","b) Schedule Trigger - d) Tumbling Window Trigger - e) Event-based Trigger","Les types de déclencheurs valides dans Azure Data Factory incluent le Schedule Trigger - Tumbling Window Trigger - et Event-based Trigger. Les déclencheurs mensuels peuvent être configurés via un Schedule Trigger - et Overlap Trigger n'existe pas.","0.0"
"Why are partition keys significant when streaming data through Azure Stream Analytics?","a) Partitions are only important on datastores - not data streams - b) The number of inbound partitions must equal the number of outbound partitions - c) They optimize OLAP operations - d) They enable parallelism and concurrency","d) They enable parallelism and concurrency","Partition keys enable parallelism and concurrency in stream processing - allowing more efficient workload distribution.","0.0"
"You need to remove data older than 36 months from a partitioned table in Azure Synapse. Which steps should you take in sequence?","a) Create an empty table - b) Switch partition to work table - c) Drop the partitioned work table","a) Create an empty table - b) Switch partition to work table - c) Drop the partitioned work table","a) Create an empty table - b) Switch partition to work table - c) Drop the partitioned work table : Correct - car cette séquence permet de retirer rapidement les données sans perturber la table principale.","0.0"
"You need to move data from a table in DB1 to a table in DB2 using a pipeline in Azure Data Factory. Which two types of objects should you create in ADF1 to complete the pipeline?","a) Linked services - b) Azure Service Bus - c) Sources and targets - d) Input and output datasets - e) Transformations","c) Sources and targets - e) Transformations","c) Sources and targets - e) Transformations : Correct - car les objets 'Sources and targets' et 'Transformations' sont nécessaires pour déplacer et transformer des données entre deux bases SQL dans une pipeline.","0.0"
"You have an Azure Databricks resource and need to log actions related to compute changes. Which service should you log?","a) Workspace - b) SSH - c) DBFS - d) Clusters","d) Clusters","d) Clusters : Correct - car les actions de calcul dans Azure Databricks - comme les changements dans les clusters - doivent être enregistrées pour la gestion des ressources.","0.0"
"You execute a pipeline that contains a stored procedure- and the procedure fails. What is the status of the pipeline run?","a) Pipeline succeeded - b) Pipeline failed - c) Pipeline skipped - d) Pipeline succeeded with failures","a) Pipeline succeeded","a) Pipeline succeeded : Correct - car même si une activité échoue - la pipeline peut être conçue pour réussir grâce à des mécanismes comme les blocs try-catch.","0.0"
"By default- how long are Azure Data Factory diagnostics logs retained?","a) 15 days - b) 30 days - c) 45 days","c) 45 days","c) 45 days : Correct - car les logs de diagnostic d'Azure Data Factory sont conservés pendant 45 jours par défaut.","0.0"
"What is the maximum number of activities per pipeline in Azure Data Factory?","a) 40 - b) 60 - c) 80 - d) 100 - e) 150","a) 40","a) 40 : Correct - car une pipeline dans Azure Data Factory peut contenir jusqu'à 40 activités - offrant ainsi une flexibilité pour les pipelines complexes.","0.0"
"Your company wants to route data rows to different streams based on matching conditions. Which transformation in mapping data flow should you use?","a) Conditional Split - b) Select - c) Lookup","a) Conditional Split","a) Conditional Split : Correct - car cette transformation permet de router les données vers différents flux selon des conditions définies.","0.0"
"You have an Azure Virtual Machine with SQL Server installed- and you need to copy data to Azure Data Lake using Data Factory. Which integration runtime should you use?","a) Azure Integration Runtime - b) Self-Hosted Integration Runtime - c) Azure SSIS Integration Runtime","b) Self-Hosted Integration Runtime","b) Self-Hosted Integration Runtime : Correct - car un runtime auto-hébergé est nécessaire lorsque vous accédez à une source de données locale - comme un serveur SQL sur une machine virtuelle.","0.0"
"You are designing a real-time dashboard for streaming data from remote sensors. What should you use for input- output- and aggregation in Azure Stream Analytics?","a) Event Hub - b) Power BI - c) Azure Stream Analytics","a) Event Hub - b) Power BI - c) Azure Stream Analytics","a) Event Hub - b) Power BI - c) Azure Stream Analytics : Corrects - car l'Event Hub est utilisé pour l'ingestion des données - Power BI pour l'affichage - et Stream Analytics pour l'agrégation des données.","0.0"
"You are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020. You need to ensure that the table meets the following requirements: Minimize the processing time to delete data that is older than 10 years- and minimize the I/O for queries that use year-to-date values. How should you complete the Transact-SQL statement?","a) Partition the table by Year - b) Partition the table by Month - c) Use a clustered columnstore index - d) Use a clustered index","a) Partition the table by Year | c) Use a clustered columnstore index","a) Partition the table by Year | c) Use a clustered columnstore index: Correct - as partitioning by year helps with deleting older data efficiently - and using a clustered columnstore index optimizes both storage and query performance for large fact tables.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following: 1 billion rows- a clustered columnstore index- a hash-distributed column named ProductKey- and a column named SalesDate that is of the date data type and cannot be null. 30 million rows will be added to Table1 each month. You need to partition Table1 based on the SalesDate column. The solution must optimize query performance and data loading. How often should you create a partition?","a) Once per month - b) Once per year - c) Once per day - d) Once per week","b) Once per year","b) Once per year: Correct - because partitioning by year balances query performance and data loading for large tables with millions of rows added each month.","0.0"
"You monitor an Apache Spark job that has been slower than usual during the last two days. The job runs a single SQL statement in which two tables are joined. You discover that one of the tables has significant data skew. You need to improve job performance. Which hint should you use in the query?","a) COALESCE - b) REBALANCE - c) REPARTITION - d) SKEW","d) SKEW","Le hint SKEW doit être utilisé pour résoudre les problèmes de déséquilibre de données (data skew) lors d'une jointure dans un travail Apache Spark. COALESCE réduit le nombre de partitions - REPARTITION spécifie le nombre de partitions basé sur des expressions - et REBALANCE ajuste la taille des partitions après une opération.","0.0"
"You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1. You need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs. What should you do?","a) Configure a global init script for workspace1 - b) Create a cluster policy in workspace1 - c) Upgrade workspace1 to the Premium pricing tier - d) Create a pool in workspace1","d) Create a pool in workspace1","d) Create a pool in workspace1: Correct - because creating a pool in Databricks allows clusters to start faster by pre-warming a set of resources - which minimizes startup time and reduces costs.","0.0"
"Which Azure Data Factory component orchestrates a transformation job or runs a data movement command?","a) Linked Services - b) Datasets - c) Activities","c) Activities","c) Activities : Correct - car les activités contiennent les étapes de transformation et de mouvement des données dans Azure Data Factory. Les autres options - comme Linked Services - ne sont que des connexions.","0.0"
"You need to monitor an Azure Synapse Analytics dedicated SQL pool to identify whether you need to scale up the service. Which metric should you monitor?","a) CPU percentage - b) Data IO percentage - c) DWU used","c) DWU used","c) DWU used : Correct - car cette métrique indique l'utilisation des unités de traitement des données - essentiel pour déterminer si un passage à un niveau de service supérieur est nécessaire.","0.0"
"You are developing a solution that uses Azure Stream Analytics to process customer data. Does the solution of having two streaming inputs- one query- and two outputs meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car la solution donnée ne répond pas aux besoins de référence de données et de requêtes multiples.","0.0"
"You have a query in Azure Stream Analytics that combines two streams of partitioned data. Is the partitioning scheme key and count required to match between the streams?","a) Yes - b) No","a) Yes","a) Yes : Correct - car les clés de partition et les comptes doivent correspondre lors de la jonction de flux de données dans Azure Stream Analytics.","0.0"
"How do you configure version control in Azure Data Factory using Git?","Based on scenario-specific configurations shown in the video","Based on scenario-specific configurations","Basé sur des configurations spécifiques : Correct - car la configuration de Git dans Azure Data Factory dépend des branches de publication et de collaboration.","0.0"
"You plan to implement a data lifecycle policy to identify and delete blobs not modified in the last 100 days. You apply an Azure Blob Storage lifecycle policy. Does this solution meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car Azure Blob Storage Lifecycle Policy permet de gérer efficacement la suppression des blobs en fonction de leur cycle de vie.","0.0"
"Which Azure data platform is commonly used to process data in an ELT framework?","a) Azure Data Factory - b) Azure Databricks - c) Azure Data Lake Storage","a) Azure Data Factory","a) Azure Data Factory : Correct - car Azure Data Factory est couramment utilisé pour traiter des données dans un cadre ELT (Extract - Load - Transform).","0.0"
"Who performs advanced analytics to drive value from data?","a) Data Engineer - b) Data Scientist - c) AI Engineer","b) Data Scientist","b) Data Scientist : Correct - car un Data Scientist effectue des analyses avancées pour extraire des informations des données. a) Data Engineer : Incorrect - car l'ingénieur de données gère l'infrastructure - pas l'analyse. c) AI Engineer : Incorrect - car il se concentre sur la construction de modèles IA - pas l'analyse de données.","0.0"
"Which transformation is used to load data into a datastore or compute resource in Azure Data Factory?","a) Source - b) Destination - c) Sync - d) Window","c) Sync","c) Sync : Correct - car la transformation Sync est utilisée pour charger les données dans une ressource de calcul ou de stockage.","0.0"
"How should Azure Data Factory be configured for best performance and schema inference when moving JSON files?","a) Merge File - b) Sync File Type: Parquet","a) Merge File - b) Parquet","a) Merge File - b) Parquet : Corrects - car fusionner les fichiers et utiliser le format Parquet offre de meilleures performances et une inférence de schéma. d'autres formats de fichiers ne sont pas aussi performants pour ces tâches.","0.0"
"You need to create an Azure Cosmos DB account that uses encryption keys managed by your organization. Which steps should you perform in sequence?","a) Create Azure Key Vault - b) Create Cosmos DB account - c) Set encryption to customer-managed key - d) Generate a new key","a) Create Azure Key Vault - b) Create Cosmos DB account - c) Set encryption to customer-managed key - d) Generate a new key","a) Create Azure Key Vault - b) Create Cosmos DB account - c) Set encryption to customer-managed key - d) Generate a new key : Corrects - car ces étapes configurent un compte Cosmos DB sécurisé avec des clés de chiffrement gérées par l'organisation.","0.0"
"You plan to create an Azure Data Lake Storage Gen2 account. You need to recommend a storage solution that meets the following requirements: First- provides the highest degree of data resiliency. Second- ensures that the container remains available for writes if a primary data center fails. What should you include in the recommendation?","a) Locally Redundant Storage - b) Geo-Redundant Storage - c) Zone-Redundant Storage - d) Read-Access Geo-Redundant Storage","b) Geo-Redundant Storage","b) Geo-Redundant Storage: Correct - because Geo-Redundant Storage ensures data is replicated across multiple data centers and provides high resiliency - ensuring data is available even if a primary data center fails.","0.0"
"You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a sliding window - and you set the window size to 10 seconds. Does this meet the goal?","a) Yes - b) No","b) No","A sliding window allows overlapping windows - meaning an event can be counted multiple times. In this case - a tumbling window is more appropriate as it ensures each event is counted exactly once.","0.0"
"You suspect there are integrity issues with the allocation of data pages in a SQL pool database. Which command should you execute to check the allocation integrity?","a) DBCC PDW_SHOWSPACEUSED - b) DBCC CHECKALLOC - c) sys.dm_pdw_node_status - d) sys.dm_pdw_nodes_db_partition_stats","b) DBCC CHECKALLOC","b) DBCC CHECKALLOC : Correct - car cette commande vérifie l'intégrité des allocations de pages dans la base de données - en s'assurant qu'il n'y a pas de corruption. Les autres options sont incorrectes : a) DBCC PDW_SHOWSPACEUSED donne des informations sur l'espace utilisé mais ne vérifie pas l'intégrité des allocations. c) et d) surveillent l'état des nœuds et des partitions - mais ne sont pas conçues pour vérifier l'intégrité des allocations de données.","0.0"
"You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and output the data to an Azure Blob Storage account. You need to output the count of tweets during the last five minutes every five minutes. Which windowing function should you use?","a) 5-minute sliding window - b) 5-minute session window - c) 5-minute tumbling window - d) 1-minute hop window","c) 5-minute tumbling window","c) 5-minute tumbling window : Correct - car cette fonction divise les flux de données en segments non chevauchants de 5 minutes pour un comptage précis des tweets.","0.0"
"Azure Databricks is?","a) data analytics platform - b) AI platform - c) Data ingestion platform","a) data analytics platform","Azure Databricks is primarily a data analytics platform that supports big data and machine learning workflows - optimized for running Apache Spark-based workloads.","0.0"
"You have a real-time data analysis solution hosted on Microsoft Azure using Event Hub and Azure Stream Analytics. How can you optimize performance for the Stream Analytics job?","a) Implement event ordering - b) Implement user-defined functions - c) Implement query parallelization by partitioning the data input - d) Scale up the SU count for the job","c) Implement query parallelization by partitioning the data input","c) Implement query parallelization by partitioning the data input : Correct - car la parallélisation améliore les performances des jobs Stream Analytics.","0.0"
"You have an Azure Data Lake Storage account with a staging zone. You need to design a daily process to ingest incremental data. Does using an Azure Data Factory schedule trigger meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car un déclencheur ne peut pas exécuter un script R directement dans le processus de pipeline.","0.0"
"You have an Azure subscription with an Azure Storage account. You need to identify and delete blobs that were not modified in the last 100 days. You scheduled an Azure Data Factory pipeline with delete activity. Does this solution meet the goal?","a) Yes - b) No","b) No","b) No : Correct - car une simple activité de suppression dans un pipeline Azure Data Factory ne suffit pas pour satisfaire les exigences de conformité.","0.0"
"You are moving data from an Azure Data Lake Gen2 storage to Azure Synapse Analytics. Which Azure Data Factory integration runtime would you use in a data copy activity?","a) Azure SSIS - b) Azure IR - c) Self-hosted - d) Pipelines","b) Azure IR","b) Azure IR : Correct - car Azure Integration Runtime permet la copie de données entre les services de stockage d'Azure et Synapse Analytics dans une région.","0.0"
"Which HDInsight cluster should you create for Lambda architecture on Azure for high-performance queries?","a) Apache HBase - b) Apache Hadoop - c) Interactive Query - d) Apache Spark","d) Apache Spark","d) Apache Spark : Correct - car Spark prend en charge les requêtes interactives et est adapté pour une architecture Lambda. a) Apache HBase - b) Apache Hadoop - c) Interactive Query : Incorrects - car ces technologies ne répondent pas aussi bien aux besoins des requêtes à faible latence.","0.0"
"Which Azure Data Factory components should you recommend using together to import daily inventory data from SQL Server to Azure Data Lake Storage?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime | a) Event-based Trigger - b) Scheduled Trigger - c) Tumbling Window Trigger | a) Copy Activity - b) Lookup Activity - c) Stored Procedure Activity","c) Self-Hosted Integration Runtime | b) Scheduled Trigger | a) Copy Activity","c) Self-Hosted Integration Runtime | b) Scheduled Trigger | a) Copy Activity : Correct - car le runtime auto-hébergé est nécessaire pour se connecter à une base SQL sur un réseau privé. Le trigger programmé permet de respecter la fréquence d'importation et l'activité de copie est utilisée pour transférer les données.","0.0"
"You have the following Azure Stream Analytics query: [Query Exhibit]. For each of the following statements- select Yes if the statement is true; otherwise- select No. First: The query combines two streams of partitioned data. Second: The stream schema key and count must match the output schema. Third: Providing 60 streaming units will optimize the performance of the query.","a) Yes - b) No","First Statement: b) No - Second Statement: a) Yes - Third Statement: a) Yes","First Statement: b) No - because the query does not combine two streams of partitioned data. Second Statement: a) Yes - because the schema key and count must match the output schema. Third Statement: a) Yes - providing sufficient streaming units optimizes query performance.","0.0"
"You configure monitoring for an Azure Synapse Analytics implementation that uses PolyBase to load data from CSV files stored in Azure Data Lake Storage Gen2 using an external table. Files with an invalid schema cause errors to occur. You need to monitor for an invalid schema error. Which error should you monitor for?","a) Storage Engine Exception - b) Schema Mismatch - c) Data Type Mismatch - d) External Table Not Found","b) Schema Mismatch","b) Schema Mismatch : Correct - car une erreur de correspondance de schéma est généralement liée à des fichiers CSV dont le format ne correspond pas aux attentes de la table externe.","0.0"
"You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb. You execute a SQL query after inserting data into mytestdb.myParquetTable using Spark. What will be returned by the following query?","a) 24 - b) an error - c) a null value","b) an error","The query will result in an error because the WHERE clause uses 'name' - which does not exist. The correct column name is 'EmployeeName'.","0.0"
"A user reports that queries against a SQL pool in Azure Synapse take longer than expected. You need to add monitoring to the underlying storage. Which two metrics should you monitor?","a) Cache Hit Percentage - b) Active Queries - c) Snapshot Storage Size - d) DWU Limit - e) Cache Use Percentage","a) Cache Hit Percentage - e) Cache Use Percentage","a) Cache Hit Percentage - e) Cache Use Percentage : Corrects - car ces métriques permettent de surveiller l'utilisation et les performances du cache - ce qui peut affecter la vitesse des requêtes.","0.0"
"You need to implement an Azure Synapse Analytics database object for storing the sales transaction data. The solution must meet the sales transaction dataset requirements. What should you do?","a) Create external table - b) Create table - c) Create view | a) Format options - b) Range left for values - c) Range right for values","b) Create table | c) Range right for values","b) Create table | c) Range right for values : Correct - car créer une table est nécessaire pour stocker les données transactionnelles et 'range right' assure que les valeurs limites appartiennent à la partition de droite - optimisant ainsi les performances de partitionnement.","0.0"
"You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub. You need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds. How should you complete the Stream Analytics query?","1) Pour compléter après FROM MessageStream : a) Last - b) Over - c) SYSTEM.TIMESTAMP() - d) TIMESTAMP BY CreatedAt | 2) Pour compléter dans GROUP BY : a) HOPPINGWINDOW - b) SESSIONWINDOW - c) SLIDINGWINDOW - d) TUMBLINGWINDOW","d) TIMESTAMP BY CreatedAt | d) TUMBLINGWINDOW","TIMESTAMP BY est utilisé pour spécifier la colonne contenant les horodatages des événements. TUMBLINGWINDOW est approprié pour segmenter les événements en fenêtres de 15 secondes sans chevauchement.","0.0"
"Your company has a branch office that contains a point of sale (POS) system. You have an Azure subscription that contains a Microsoft SQL Server database named DB1 and an Azure Synapse Analytics workspace. You plan to use an Azure Synapse pipeline to copy CSV files from the branch office- perform complex transformations on their content- and then load them to DB1. You need to pass a subset of data to test whether the CSV columns are mapped correctly. What can you use to perform the test?","a) Data Flow Debug - b) Datasets - c) Integration runtime - d) Linked service","a) Data Flow Debug","L'option Data Flow Debug est disponible dans une activité de flux de données et permet de passer un sous-ensemble de données à travers le flux - ce qui peut être utile pour tester si les colonnes sont correctement mappées. - -Les autres options ne sont pas correctes car : -- **b) Datasets** : fait référence aux données spécifiques consommées et produites par les activités dans un pipeline - et non à une option de test. -- **c) Integration runtime** : est un concept de pipeline qui fait référence aux ressources de calcul nécessaires pour exécuter le pipeline - et non un outil de test. -- **d) Linked service** : est requis lorsqu'une activité a besoin ou dépend d'un service externe - mais il ne s'agit pas d'un outil pour tester le mappage de colonnes.","0.0"
"Which technology is a globally distributed- multi-model database with sub-second query performance?","a) SQL Database - b) Azure SQL Database - c) Apache Hadoop - d) Cosmos DB - e) Azure Synapse","d) Cosmos DB","d) Cosmos DB : Correct - car Cosmos DB est une base de données mondiale multi-modèle avec des performances de requête en temps réel. a) SQL Database - b) Azure SQL Database - c) Apache Hadoop - e) Azure Synapse : Incorrects - car ils ne sont pas conçus pour des performances globales aussi rapides que Cosmos DB.","0.0"
"In a dedicated Synapse Analytics SQL pool- you want to know the space used by a specific table in the database. Which command should you execute to retrieve this information?","a) DBCC PDW_SHOWSPACEUSED - b) DBCC CHECKALLOC - c) sys.dm_pdw_node_status - d) sys.dm_pdw_nodes_db_partition_stats","a) DBCC PDW_SHOWSPACEUSED","a) DBCC PDW_SHOWSPACEUSED : Correct - car cette commande permet d'obtenir des informations sur l'espace utilisé par une table spécifique dans un pool SQL dédié. Les autres options sont incorrectes : b) DBCC CHECKALLOC vérifie l'intégrité des allocations de pages - mais ne fournit pas d'informations sur l'espace utilisé. c) et d) sont des vues système pour surveiller l'état des nœuds et des partitions - mais elles ne donnent pas de détails sur l'espace utilisé par une table.","0.0"
"Microsoft Purview est une solution de gouvernance des données qui aide à gérer et protéger les actifs de données.","Le portail de gouvernance est utilisé pour rechercher et gérer les métadonnées dans Microsoft Purview.","Le portail de conformité est conçu pour la gestion des risques liés aux informations sensibles.","Le portail de gouvernance est essentiel pour identifier les actifs catalogués, contrairement aux autres services Azure.","0.0"
"You have an Azure subscription and plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool1 that will contain staging tables and a dimensional model. Pool1 will contain the following tables: You need to design the table storage for pool1. The solution must meet the following requirements: First- maximize the performance of data loading operations to the staging tables. Second- minimize query times for reporting queries against the dimensional model. Which type of table distribution should you use for each table?","a) Hash - b) Round-robin - c) Replicated","Staging Tables: b) Round-robin - Dimensional Tables: a) Hash","Staging Tables: b) Round-robin - Dimensional Tables: a) Hash: Correct - because round-robin maximizes performance for staging tables - and hash distribution optimizes query performance for the dimensional model.","0.0"
"You monitor an Azure Stream Analytics job and discover that the Backlogged Input Events metrics show non-zero values for the last few hours. What should you do to improve job performance without changing the query?","a) Increase the number of Streaming Units (SU). - b) Increase the settings for late events. - c) Move the job to the dedicated Stream Analytics cluster. - d) Repartition the input stream.","a) Increase the number of Streaming Units (SU).","Vous devez augmenter le nombre d'unités de streaming (SUs) - car cela ajoute plus de puissance de calcul au travail. L'augmentation des paramètres d'événements en retard ne résoudra pas le problème de performance - et le déplacement du travail vers un cluster dédié donne un accès administratif mais n'améliore pas la performance directement.","0.0"
"You are designing a Synapse SQL pool and need to audit access to PII (personally identifiable information). What should you implement?","a) Column-Level Security - b) Dynamic Data Masking - c) Row-Level Security - d) Sensitivity Classification","d) Sensitivity Classification","d) Sensitivity Classification : Correct - car elle permet d'auditer l'accès aux informations personnellement identifiables (PII).","0.0"
"You have an Azure Synapse Analytics Apache Spark pool named pool1. You plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in pool1. The structure and data types vary by file. You need to load the files into the tables- and the solution must maintain the source data types. What should you do?","a) Use a conditional split transformation in Azure Synapse Data Flows - b) Use a Get Metadata activity in Azure Data Factory - c) Load the data by using the OPENROWSET Transact-SQL command in an Azure Synapse Analytics serverless SQL pool - d) Load the data by using PySpark","d) Load the data by using PySpark","d) Load the data by using PySpark: Correct - because PySpark allows flexible schema inference and can handle varying structures and data types while maintaining the source data types.","0.0"
"You have an Enterprise Data Warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012. You need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must create four partitions based on the order date and ensure that each partition contains all orders placed during a given calendar year. How should you complete the T-SQL command?","a) RIGHT - b) LEFT | a) VALUES (2009 - 2010 - 2011 - 2012) - b) VALUES (2013 - 2014 - 2015 - 2016) - c) VALUES (2012 - 2013 - 2014 - 2015)","b) LEFT | a) VALUES (2009 - 2010 - 2011 - 2012)","b) LEFT | a) VALUES (2009 - 2010 - 2011 - 2012) : Correct - car la clause LEFT partitionne les valeurs de l'année à gauche et les valeurs choisies permettent de diviser les données par année civile - comme exigé.","0.0"
"Publishers can use either HTTPS or AMQP to send messages to Azure Event Hub. How many default partitions are available?","a) 1 - b) 2 - c) 4 - d) 8 - e) 12","c) 4","c) 4 : Correct - car par défaut - Azure Event Hub fournit quatre partitions.","0.0"
"What information is displayed as output when you execute the PDW_SHOWSPACEUSED command?","a) The number of rows on each table for each distribution - b) The amount of physical space used for the table on each distribution - c) The amount of data read from cache for the given table per distribution - d) The amount of remaining space in the distribution","a) The number of rows on each table for each distribution - b) The amount of physical space used for the table on each distribution","The command shows the number of rows and the physical space used for the table on each distribution.","0.0"
"You plan to implement an Azure Data Lake Gen 2 storage account and need to ensure that it remains available even if a data center fails in the primary Azure region. Which type of replication should you use?","a) Geo-Redundant Storage (GRS) - b) Geo-Zone Redundant Storage (GZRS) - c) Zone Redundant Storage (ZRS) - d) Locally Redundant Storage (LRS)","c) Zone Redundant Storage (ZRS)","c) ZRS : Correct - car il assure une redondance à moindre coût dans la région primaire même en cas de panne d'un centre de données.","0.0"
"You are designing a financial transaction table in Azure Synapse Analytics with 500 million rows per account type and 65 million rows per transaction month. Analysts will mostly query data by month. On which column should you partition the table?","a) Customer Segment - b) Account Type - c) Transaction Type - d) Transaction Month","d) Transaction Month","d) Transaction Month : Correct - car partitionner par mois minimise les temps de requête pour les analyses par période.","0.0"
"You have an Azure Synapse Analytics dedicated SQL pool with a large fact table. Incoming queries use the sales key column. Which distribution method should you use to optimize performance?","a) Hash Distributed with Clustered Index - b) Hash Distributed with Clustered Columnstore Index - c) Round Robin Distributed with Clustered Index - d) Round Robin Distributed with Clustered Columnstore Index","b) Hash Distributed with Clustered Columnstore Index","b) Hash Distributed with Clustered Columnstore Index : Correct - car cela permet d'optimiser les performances des requêtes sur de grandes tables de faits.","0.0"
"You want to ingest data from a SQL Server database hosted on an on-premises Windows server using Azure Data Factory. Which integration runtime is required?","a) Azure Integration Runtime - b) Azure SSIS Integration Runtime - c) Self-Hosted Integration Runtime","c) Self-Hosted Integration Runtime","c) Self-Hosted Integration Runtime : Correct - car un runtime auto-hébergé est nécessaire pour accéder à un serveur SQL local depuis Azure Data Factory.","0.0"
"Les Streaming Units (SU) dans Azure Stream Analytics représentent la puissance de calcul allouée à une tâche.","Les Backlogged Input Events se produisent lorsque les événements d'entrée ne sont pas traités aussi rapidement qu'ils sont ingérés.","Lorsque vous voyez des Backlogged Input Events - augmentez le nombre de SUs pour ajouter plus de puissance de calcul à la tâche.","Les autres options - comme augmenter les événements en retard ou déplacer la tâche vers un cluster dédié - ne résolvent pas directement le problème de performance sans changer la requête.","0.0"
"Which of the following data file formats offers the highest performance and flexibility for data analytics?","a) Yet Another Markup Language (YAML) - b) Extensible Markup Language (XML) - c) Apache Parquet - d) JavaScript Object Notation (JSON)","c) Apache Parquet","Apache Parquet is optimized for big data processing and analytics - offering high performance for columnar storage.","0.0"
"You have source data that contains an array of JSON objects. Each JSON object has a child array of JSON objects. You create a data flow activity in an Azure Synapse Analytics pipeline. You need to transform the source so that it can be written to an Azure SQL Database table where each row represents an element of the child array- along with the values of its parent element. Which type of task should you add to the data flow activity?","a) flatten - b) parse - c) pivot - d) unpivot","a) flatten","La tâche flatten permet d'aplatir les tableaux JSON en transformant chaque élément de l'enfant dans une ligne avec les valeurs de son parent. La tâche parse est utilisée pour analyser des données brutes - tandis que pivot et unpivot créent des colonnes ou des lignes à partir des valeurs existantes - ce qui ne convient pas pour ce cas de figure.","0.0"
"You need to verify the size of transaction logs for each distribution in an Azure Synapse Analytics data warehouse. Which query should you execute?","a) Query sys.database_files in master - b) Query Azure Monitor logs - c) Query sys.transaction_logs in DW1","a) Query sys.database_files in master","a) Query sys.database_files in master : Correct - car cette vue permet de surveiller la taille des fichiers journaux de transaction.","0.0"
"You are using PolyBase in Azure Synapse to query Parquet files. You need to add a new column to the external table. What should you do?","a) Alter External Table - b) Drop and Create External Table","b) Drop and Create External Table","b) Drop and Create External Table : Correct - car il faut d'abord supprimer puis recréer la table externe pour ajouter la nouvelle colonne.","0.0"
"Which browsers are recommended for the best use of Azure Databricks?","a) Google Chrome - b) Firefox - c) Safari - d) Microsoft Edge - e) Internet Explorer - f) Mobile Browsers","a) Google Chrome - b) Firefox - c) Safari - d) Microsoft Edge","a) - b) - c) - d) : Corrects - car Google Chrome - Firefox - Safari - et Microsoft Edge sont recommandés par Microsoft. e) Internet Explorer et f) Mobile Browsers : Incorrects - car ils ne sont pas recommandés pour une utilisation optimale de Databricks.","0.0"
"You have an Azure Synapse Analytics workspace. You need to build a materialized view. Which two items should be included in the SELECT clause of the view? Each correct answer presents part of the solution.","a) a subquery - b) an aggregate function - c) the GROUP BY clause - d) the HAVING clause - e) the OPTION clause","b) an aggregate function - c) the GROUP BY clause","Lors de la création d'une vue matérialisée dans Azure Synapse Analytics - la clause SELECT doit inclure au moins une fonction d'agrégation et la clause GROUP BY correspondante. Les autres options comme la clause HAVING - une sous-requête - ou la clause OPTION ne sont pas obligatoires pour construire une vue matérialisée.","0.0"
"You have an Azure subscription containing an Azure Data Lake Storage account named MyAccount1. MyAccount1 contains two containers: Container1 and Container2. The subscription is linked to an Azure Active Directory tenant that contains a security group named Group1. You need to grant Group1 read access to Container1. The solution must use the principle of least privilege. Which role should you assign to Group1?","a) Storage Blob Data Reader for Container1 - b) Storage Table Data Reader for Container1 - c) Storage Blob Data Reader for MyAccount1 - d) Storage Table Data Reader for MyAccount1","a) Storage Blob Data Reader for Container1","a) Storage Blob Data Reader for Container1 : Correct - car ce rôle accorde uniquement les permissions nécessaires pour lire les données dans Container1 - conformément au principe du moindre privilège.","0.0"
"You develop a data ingestion process that will import data to Microsoft Azure SQL Data Warehouse. The data to be ingested resides in Parquet files stored in an Azure Data Lake Gen2 storage account. You need to load the data from the Azure Data Lake Gen2 storage account into the Azure SQL Data Warehouse. The solution is to create an external data source pointing to the Azure Data Lake Gen2 storage account- create an external file format and external table using the external data source- and load the data using CREATE TABLE AS SELECT statement. Does this meet the goal?","a) Yes - b) No","a) Yes","a) Yes : Correct - car l'instruction CREATE TABLE AS SELECT est la méthode recommandée pour charger des données depuis Azure Data Lake Gen2 dans un pool SQL dédié dans Synapse Analytics.","0.0"
"Which transformation is used to load data into a datastore or compute resource in Azure Data Factory?","a) Source - b) Destination - c) Sync - d) Window","c) Sync","c) Sync : Correct - car la transformation Sync est utilisée pour charger les données dans une ressource de calcul ou de stockage.","0.0"
"You have a SQL pool in Azure Synapse that contains a table named `dbo.customers`. You need to mask the email column to prevent non-administrative users from seeing full email addresses. What should you use?","a) From SQL Server Management Studio - set an email mask on the email column - b) From the Azure Portal - set a mask on the email column - c) From SQL Server Management Studio - grant select permission to users for all columns except email - d) From the Azure Portal - set a sensitivity classification of confidential for the email column","b) From the Azure Portal - set a mask on the email column","b) Set a mask on the email column : Correct - car cela masque les données sensibles tout en permettant l'accès aux utilisateurs. a) - c) - d) : Incorrects - car ils ne masquent pas correctement les données dans tous les scénarios.","0.0"
"You are designing a data partitioning strategy for an Azure Synapse Analytics workload. You need to implement a distribution strategy that optimizes data load operations. Which type of distribution should you use?","a) Hash - b) replicated tables - c) round-robin","c) round-robin","Le round-robin est optimisé pour le chargement des données. Le hachage optimise uniquement les requêtes - tandis que les tables répliquées offrent des performances de requête optimisées pour les petites tables.","0.0"
"Your company wants to route data rows to different streams based on matching conditions. Which transformation in mapping data flow should you use?","a) Conditional Split - b) Select - c) Lookup","a) Conditional Split","a) Conditional Split : Correct - car cette transformation permet de router les données vers différents flux selon des conditions définies.","0.0"
"La friction mène à la croissance.","Utilisé pour décrire comment les difficultés conduisent à la croissance personnelle.","La friction dans la vie provoque la croissance - tout comme dans l'entraînement physique.","Rechercher des tâches difficiles ou des défis pour favoriser le développement personnel.",""
"L'effort difficile construit le cortex cingulaire antérieur médian.","Illustre le processus de construction de la résilience mentale à travers des tâches difficiles.","La lutte entraîne des changements dans le cerveau qui renforcent la personne mentalement.","S'engager dans des tâches que tu n'aimes pas pour construire la résilience et la force mentale.",""
"Reste dur.","Encouragement à maintenir une dureté mentale face aux défis.","Un appel à une autodiscipline constante face aux difficultés.","Appliquer cet état d'esprit quotidiennement pour maintenir la discipline et continuer à s'améliorer.",""
"Tu crées ton propre moi.","Autodétermination et responsabilité dans la construction de sa vie.","Tu es responsable de la personne que tu deviens.","Prendre la responsabilité de façonner ton avenir à travers des actions cohérentes.",""
"Il n'y a pas de potion magique- il suffit d'agir.","Décrire que la seule voie est l'action - ne pas attendre la motivation.","Le succès vient des efforts répétés - non de l'attente de l'inspiration.","Se concentrer sur le travail acharné plutôt que d'attendre le bon moment pour agir.",""
"Le pouvoir de la discipline et du travail acharné","David Goggins explique que tout ce qu'il a accompli est le résultat d'une discipline et d'un effort constant.","La discipline et l'effort constant sont plus importants que le talent.","Adopter une routine stricte pour développer la discipline.",""
"La transformation par la souffrance","Goggins met l'accent sur la souffrance comme outil de transformation personnelle.","La souffrance est un catalyseur pour la croissance et la transformation.","Accepter et affronter les moments difficiles pour se transformer.",""
"L'importance de se confronter à soi-même","Il parle de la nécessité d'explorer ses propres limites pour se découvrir.","Se confronter à soi-même permet de mieux se comprendre.","Réfléchir à ses propres limites et les dépasser.",""
"Le concept de 'No Days Off' (aucun jour de repos)","L'idée qu'il faut continuer à travailler - même dans la douleur - sans jours de repos.","Le travail quotidien est essentiel à la réussite et au dépassement de soi.","Ne pas relâcher ses efforts - même lorsque l'on atteint un objectif.",""
"La résilience mentale","Renforcer la résilience mentale est un processus quotidien - et peut régresser si on n'affronte plus de défis.","La résilience mentale doit être entraînée en permanence.","Mettre en place des défis réguliers pour entraîner sa résilience mentale.",""
"L'importance de l'autodiscipline pour atteindre la grandeur","Goggins rejette les raccourcis et insiste sur la rigueur pour atteindre la grandeur.","L'autodiscipline est la clé pour atteindre le succès à long terme.","Pratiquer une autodiscipline rigoureuse sans céder aux tentations.",""
"Le futur appartient à ceux qui croient à la beauté de leurs rêves.","Motivation personnelle pour poursuivre ses objectifs.","Croyance dans ses rêves = moteur pour atteindre ses objectifs.","Cette punchline illustre l'importance de la foi en ses rêves et aspirations comme facteur clé de réussite. Se concentrer sur ses ambitions permet de les réaliser.",""
"Ce n'est pas la taille du chien dans le combat- mais la taille du combat dans le chien.","Encouragement dans un contexte de compétition.","La force de la détermination prévaut sur les limites physiques.","Elle montre que ce qui compte n'est pas la taille physique - mais la volonté de réussir. C'est une leçon de détermination face aux défis.",""
"Le succès- c'est se promener d'échec en échec tout en restant motivé.","Motiver les gens à persister face à l'échec.","Chaque échec est une opportunité pour grandir et avancer.","L'idée ici est que l'échec n'est pas final. Chaque échec est une chance de s'améliorer et de trouver de nouvelles solutions pour avancer.",""
"Le succès n’est pas la clé du bonheur- le bonheur est la clé du succès.","Montrer que le bonheur précède souvent le succès.","Le bonheur intrinsèque permet de réussir dans ce que l'on entreprend.","Il ne faut pas courir après le succès pour être heureux. Si vous êtes heureux dans ce que vous faites - le succès viendra naturellement.",""
"Un pessimiste voit la difficulté dans chaque opportunité ; un optimiste voit l'opportunité dans chaque difficulté.","Encourager une attitude positive face aux défis.","Une attitude positive transforme les défis en opportunités de croissance.","Ce proverbe oppose l'attitude pessimiste - qui voit seulement les problèmes - à l'attitude optimiste - qui voit des opportunités d'amélioration dans chaque défi.",""
"Celui qui déplace la montagne commence par de petites pierres.","Illustration de la persévérance dans l'accomplissement de grandes tâches.","Les grands objectifs sont atteints par de petites actions cohérentes.","Il s'agit de rappeler que chaque grande réalisation commence par de petits efforts. La patience et la persévérance sont essentielles.",""
"Goggins parle de la nécessité de créer une réalité alternative pour se motiver- en dépit de sa réalité actuelle.","Créer une fausse réalité pour dépasser ses limitations actuelles.","Créer une nouvelle réalité pour se motiver à dépasser ses limites.","Goggins mentionne qu'on doit se créer une fausse réalité pour trouver la motivation de changer.",""
"Huberman raconte comment une seule conversation peut provoquer des changements radicaux chez certaines personnes.","Un ami a perdu 60 livres et a arrêté de boire après une simple conversation.","Certaines personnes peuvent changer radicalement avec une simple conversation.","Huberman parle d'un exemple où un ami a totalement changé sa vie après une seule conversation sur la discipline.",""
"Goggins insiste sur l'idée que tout le monde sait ce qu'il doit faire- mais la difficulté réside dans l'exécution.","Les gens échouent parce qu'ils ne font pas ce qu'ils savent déjà devoir faire.","Tout le monde sait ce qu'il doit faire - mais l'exécution est la clé.","Goggins répète plusieurs fois que tout le monde sait déjà quoi faire - mais que la difficulté réside dans le passage à l'acte.",""
"Goggins met en évidence l'importance d'agir immédiatement- sans attendre un moment parfait.","Il faut commencer maintenant - peu importe les circonstances.","Ne jamais attendre l'heure parfaite pour agir.","Goggins met en garde contre l'attente d'un moment parfait et insiste sur l'importance de démarrer maintenant.",""
"Goggins parle de l'importance d'être prêt à affronter la douleur et l'échec pour avancer dans la vie.","Être prêt à souffrir et à répéter les efforts pour réussir.","Le succès vient avec l'acceptation de la douleur et de l'échec.","Il met en avant la nécessité de répéter les efforts et de s'habituer à l'inconfort pour atteindre la réussite.",""
"Huberman confie ses luttes personnelles avec la discipline et comment il doit changer sa méthode pour démarrer les choses.","L'importance de commencer à n'importe quel moment - même si ce n'est pas l'heure parfaite.","Changer ses habitudes de procrastination pour agir immédiatement.","Huberman partage une lutte personnelle avec la procrastination et comment il doit changer sa façon de démarrer les choses.",""
"Goggins explique que la confiance se construit à travers des efforts constants et une confrontation avec ses démons.","La confiance se gagne par la répétition et le fait de surmonter ses insécurités.","La confiance naît des actions répétées face aux difficultés.","Goggins partage comment il a dû affronter ses insécurités et construire sa confiance à travers la répétition et l'effort.",""
